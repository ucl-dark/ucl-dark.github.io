



<!DOCTYPE html>
<html lang="en">
  <head>
    

    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="stylesheet" href="static/css/main.css" type="text/css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="icon" href="static/images/logo.png">

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
    <script src="https://kit.fontawesome.com/c59ce62110.js" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />

    <title>UCL DARK Lab: Schedule</title>
    
<script src="https://cdn.jsdelivr.net/npm/ical.js@1.4.0/build/ical.min.js" integrity="sha256-iFbWWsVYU6rIgbq5wJc4mp8zvV3SkqupxgH/7a9PanY=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.26.0/dist/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.31/builds/moment-timezone-with-data-10-year-range.min.js" integrity="sha256-Dx3P9LwbB/WuS+7Xv37Y+qcPS/14AwFH653Pw80AOhY=" crossorigin="anonymous"></script>
<link
  rel="stylesheet"
  type="text/css"
  href="https://cdn.jsdelivr.net/npm/tui-calendar@1/dist/tui-calendar.min.css"
/>

<script src="https://cdn.jsdelivr.net/npm/tui-code-snippet@1.5.2/dist/tui-code-snippet.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/tui-dom@3.0.0/dist/tui-dom.min.js" integrity="sha256-Ha3QUAK/d/Z2BMN/hDoBjwP0Q0f5uS9MploTRSLqrLI=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/tui-calendar@1/dist/tui-calendar.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.10/dist/css/bootstrap-select.min.css"
/>
<!-- Latest compiled and minified JavaScript -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.10/dist/js/bootstrap-select.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/lodash@4.17.15/lodash.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/js-cookie@rc/dist/js.cookie.min.js"></script>

<script src="static/js/lazy_load.js"></script>
<script src="static/js/little_helpers.js"></script>
<script type="text/javascript" src="static/js/calendar.js"></script>

  </head>

  <body>
    <!-- NAV -->
    
    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
      id="main-nav"
    >
        <div class="container">
        <!--
        <a class="navbar-brand" href="index.html">
          <img
             class="logo" src="static/images/logo.png"
             height="auto"
             width="130px"
          />
        </a>
        -->
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="index.html">Home</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Publications</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="speakers.html">Speakers</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
        
<ul class="nav nav-pills justify-content-center">
  
  <li class="nav-item">
    <a
      class="nav-link active text-muted "
      data-toggle="tab"
      href="#tab-calendar"
      role="tab"
      aria-controls="nav-home"
      aria-selected="true"
      >Calendar
    </a>
  </li>
  
  <li class="nav-item">
    <a
      class="nav-link  text-muted "
      data-toggle="tab"
      href="#tab-day"
      role="tab"
      aria-controls="nav-home"
      aria-selected="true"
      >Example Day
    </a>
  </li>
  
</ul>

      </div>
      <!-- Content -->
      <div class="content">
        
<div class="tab-content py-3 px-3 px-sm-0" id="nav-tabContent">
  <!-- Calender tab -->
  <div
    class="tab-pane active"
    id="tab-calendar"
    role="tabpanel"
    aria-labelledby="nav-profile-tab"
  >
    <div class="form-group col">
      <label for="tzOptions">Timezone:</label>
      <select id="tzOptions" class="selectpicker" data-live-search="true">
      </select>
    </div>

    <!-- full cal for browser-->
    <div id="calendar" class="d-none d-sm-block"></div>

    <!-- small cal for smart phones-->
    <div id="calendar_small" class="d-sm-none"></div>

    <script type="text/javascript">
      make_cal("serve_main_calendar.json");
    </script>
  </div>

  <!-- Day Tab -->

  <div
    class="tab-pane fade"
    id="tab-day"
    role="tabpanel"
    aria-labelledby="nav-profile-tab"
  >
    <div id="day">
      <!-- Speakers -->
      <div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Speakers</h2>
  </div>
</div>
      <div class="speakers">
        
<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_21.html" class="main-title">
                  Lifelong Learning for Robotics
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Franziska Meier  /  Meta AI
              </span>
                <span class="card-title h3">
                March 24, 2022
              </span>
            </div>
            <div class="m-3">
              While there has been major investment in developing robot learning algorithms, achieving true autonomy remains a wide-open research question. A key limitation of current learning approaches is the assumption that learning is a one-time event on a pre-defined task-distribution and that an expert can manually specify how a robot should learn these tasks from multi-modal sensor data. This approach to setting up the learning problem fundamentally hinders robots from being able to learn new tasks or adapt to new situations autonomously. In this talk, I will discuss the challenges of the frontier of a lifelong learning robot and what algorithmic advancements are required to advance the state-of-the-art in autonomous robotics. In short, progress towards lifelong learning robots requires algorithms that enable a robot to learn new skills incrementally and continuously (without forgetting), autonomously (without expert intervention) and sample-efficiently. In this context, I will present our recent advances towards autonomous learning of robotic manipulation skills by enabling the robot to learn reward functions while only requiring minimal expert intervention. Specifically, I will present a unified framework for model-based and model-free reward learning algorithms, that can either learn rewards from demonstrations or under-specified (sparse) rewards.  Finally, I will discuss the challenge of learning rewards that generalize to novel settings, and present initial insights from our empirical analysis of how well learned rewards generalize.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_20.html" class="main-title">
                  Understanding the World Through Action
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Sergey Levine  /  UC Berkeley
              </span>
                <span class="card-title h3">
                January 6, 2022
              </span>
            </div>
            <div class="m-3">
              The capabilities of modern machine learning systems are to a large extent determined by their ability to effectively utilize large and diverse datasets. However, such systems typically focus on making predictions rather than making decisions, with the aim of maximizing the likelihood of some data rather than a user-specified utility function. Reinforcement learning methods directly address the problem of utility maximization, but such methods are difficult to reconcile with modern data-driven learning, and typically require either active data collection or specially tailored datasets, both of which are not conducive for being able to use large datasets. In this talk, I will discuss how learning-based control can be performed with offline data, and how such offline RL algorithms can utilize comparatively less specialized datasets with general-purpose objectives to enable learning to make decisions at scale. I will discuss the algorithmic foundations of offline reinforcement learning, and present a number of robotics applications that use comparatively general data sources in the areas of robotic navigation and manipulation.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_19.html" class="main-title">
                  Learning Structured Models of the World
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Thomas Kipf  /  Google Brain
              </span>
                <span class="card-title h3">
                December 16, 2021
              </span>
            </div>
            <div class="m-3">
              The world around us — and our understanding of it — is rich in compositional structure: from atoms and their interactions to objects and entities in our environments. How can we learn models of the world that take this structure into account and generalize to new compositions in systematic ways? This talk focuses on an emerging class of slot-based neural architectures that can discover, represent, and reason about abstract entities from perceptual input alone. Taking our recent work on Slot Attention as an example, I will explain the challenges for object discovery and how attention-based routing provides an elegant solution to mapping from low-level perceptual features to high-level object-centric abstractions. With Slot Attention for Video (SAVi), we extend this framework to temporally-consistent modeling of objects over time and show how information about object motion can help the model find the right decomposition of a scene into its constituent components. Finally, I will discuss some of the open challenges that remain for developing and deploying structured world models.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_18.html" class="main-title">
                  Open-Ended Learning Leads to Generally Capable Agents
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Max Jaderberg  /  DeepMind
              </span>
                <span class="card-title h3">
                November 25, 2021
              </span>
            </div>
            <div class="m-3">
              In this talk I will cover our recent publication "Open-Ended Learning Leads to Generally Capable Agents". In this work we turn our attention to how to create embodied agents in simulation that can generalise to unseen test tasks and exhibit generally capable behaviour. I will introduce our XLand procedurally generated environment, and the open-ended learning algorithms that allow us to train agents to cover this vast environment space. This results in agents that are capable across a wide range of held-out test tasks including hide-and-seek and capture-the-flag, and we will explore these results and the emergent behaviours and representations of the agent.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_17.html" class="main-title">
                  Training Virtual Robots in Realistic Simulators
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Erik Wijmans  /  Georgia Institute of Technology
              </span>
                <span class="card-title h3">
                September 2, 2021
              </span>
            </div>
            <div class="m-3">
              Recently there has been a shift in computer vision research from static vision, e.g., object detection, to Embodied AI, e.g., robot navigation. In this talk, I will focus on the task of PointGoal navigation (PointNav) where an embodied agent (virtual robot) is tasked with navigating to a point specified relative to its initial location in an unknown environment and without a map. I will ask and answer: Given only a depth camera and localization sensor, is this task learnable with generic tools, model-free reinforcement learning (RL) and generic neural networks? To answer this question, my collaborators and I developed a new distributed system for RL designed to meet the needs of training in realistic simulation. We showed that this task is entirely learnable by training an agent for the equivalent of 80 years of human experience. We then designed a new simulation paradigm specifically for the needs of RL centered on large batch simulation, where the simulator simulates many agents in many environments at once and is responsible for its own parallelization, reducing training wall-clock time from 6 GPU months to 36 GPU hours, an over 100x improvement.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_16.html" class="main-title">
                  Towards Human-Like And Collaborative AI in Video Games
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Katja Hofmann  /  Microsoft Research
              </span>
                <span class="card-title h3">
                August 19, 2021
              </span>
            </div>
            <div class="m-3">
              Developing agents capable of learning complex human-like behaviors is a key goal of artificial intelligence research. Progress towards this goal has exciting potential for applications in video games, from new tools that empower game developers to realize new creative visions, to enabling new kinds of immersive player experiences. In this talk I will share insights recently developed by my team and myself on how reinforcement learning, and other AI techniques can give rise to more human-like bot or NPC behaviors, focusing on the following components. First, I motivate the need for accurately evaluating human-likeness, propose a solution within a single-agent navigation task, and show that achieving highly skilled behavior is insufficient for human-likeness. Second, I demonstrate the first agent that passes our bar for human-likeness. Finally, I discuss how to go beyond single agent tasks and towards learning to collaborate, using structured models for predicting other agents’ behavior.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_15.html" class="main-title">
                  Towards Knowledge Grounded Text Agents
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Matthew Hausknecht  /  Microsoft Research
              </span>
                <span class="card-title h3">
                July 8, 2021
              </span>
            </div>
            <div class="m-3">
              Learning agents in text-based games are faced with the challenge of understanding and generating language to accomplish various goals. Decomposing this primary challenge, I will highlight recent work to address sub-challenges of knowledge representation, affordance detection, commonsense reasoning, and language grounding. Specifically, I will discuss knowledge graphs as a means of tracking agent state, language models as generators for valid actions, and ALFWorld, a new environment that aligns text and embodied modalities for the study of language grounding.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_14.html" class="main-title">
                  Using Video Games To Reverse Engineer Human Intelligence
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Sam Gershman  /  Harvard University
              </span>
                <span class="card-title h3">
                June 21, 2021
              </span>
            </div>
            <div class="m-3">
              Video games have become an attractive testbed for evaluating AI systems, by capturing some aspects of real-world complexity (rich visual stimuli and non-trivial decision policies) while abstracting away from other sources of complexity (e.g., sensory transduction and motor planning). Some AI researchers have reported human-level performance of their systems, but we still have very little insight into how humans actually learn to play video games. This talk will present new data on human video game learning indicating that humans learn very differently from most current AI systems, particularly those based on deep learning. Humans can induce object-oriented, relational models from a small amount of experience, which allow them to learn quickly, explore intelligently, plan efficiently, and generalize flexibly. These aspects of human-like learning can be captured by a model that learns through a form of program induction.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_13.html" class="main-title">
                  Exploring Context for Better Generalization in Reinforcement Learning
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Amy Zhang  /  UC Berkeley and Facebook AI Research
              </span>
                <span class="card-title h3">
                June 7, 2021
              </span>
            </div>
            <div class="m-3">
              The benefit of multi-task learning over single-task learning relies on the ability to use relations across tasks to improve performance on any single task. While sharing representations is an important mechanism to share information across tasks, its success depends on how well the structure underlying the tasks is captured. In some real-world situations, we have access to metadata, or additional information about a task, that may not provide any new insight in the context of a single task setup alone but inform relations across multiple tasks. While this metadata can be useful for improving multi-task learning performance, effectively incorporating it can be an additional challenge. In this talk, we explore various ways to utilize context to improve positive transfer in multi-task reinforcement learning.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_12.html" class="main-title">
                  Some Reasons To Do Embodied Machine Learning
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Felix Hill  /  DeepMind
              </span>
                <span class="card-title h3">
                May 24, 2021
              </span>
            </div>
            <div class="m-3">
              In this talk, I'll give four good reasons to do embodied machine learning. Learning in an agent that can perceive and interact with its environment is fundamentally different from other ML settings. Unlike a disembodied model, an embodied learner must learn to perceive and move in addition to mastering whatever specific behaviour the user may be interested in. This may seem like a disadvantage, because there is in some sense more to learn, but it can also be an advantage when trying to replicate human cognitive behaviours like reasoning and generalization. Having a body and being necessarily located at a specific place at a given time places strong constraints on the learner's experience, which in turn leads to more human-like learning outcomes. These results suggest that embodied learning may be an important part of what is needed to convincingly replicate human linguistic intuitions and behaviours in a machine.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_11.html" class="main-title">
                  Mental Simulation, Imagination, and Model-Based Deep RL
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Jessica Hamrick  /  DeepMind
              </span>
                <span class="card-title h3">
                May 10, 2021
              </span>
            </div>
            <div class="m-3">
              Mental simulation—the capacity to imagine what will or what could be—is a salient feature of human cognition, playing a key role in a wide range of cognitive abilities. In artificial intelligence, the last few years have seen the development of model-based deep reinforcement learning methods, which seemingly share many similarities with mental simulation. In this talk, I will discuss how closely such methods actually capture the qualitative characteristics exhibited by human mental simulation, with a particular focus on: (1) the extent to which the performance of such agents is driven by model-based reasoning and planning, and (2) how effectively such agents can leverage planning for generalization. While a number of challenges remain in matching the capacity of human mental simulation, I will highlight some recent progress on developing more compositional model-based algorithms through the use of graph neural networks and tree search.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_10.html" class="main-title">
                  Towards multi-agent emergent communication as a building block of human-centric AI
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Angeliki Lazaridou  /  DeepMind
              </span>
                <span class="card-title h3">
                Apr 26, 2021
              </span>
            </div>
            <div class="m-3">
              The ability to cooperate through language is a defining feature of humans. As the perceptual, motory and planning capabilities of deep artificial networks increase, researchers are studying whether they can also develop a shared language to interact. In this talk, I will highlight recent advances in this field but also common headaches (or perhaps limitations) with respect to experimental setup and evaluation of emergent communication. Towards making multi-agent communication a building block of human-centric AI, and by drawing from my own recent work, I will discuss approaches on making emergent communication relevant for human-agent communication in natural language.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_9.html" class="main-title">
                  Finding Good Representation for Search and Exploration in RL
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Yuandong Tian  /  Facebook AI Research
              </span>
                <span class="card-title h3">
                Apr 12, 2021
              </span>
            </div>
            <div class="m-3">
              How to learn good latent representations is an important topic in the modern era of machine learning. For reinforcement learning, using a good representation makes the decision-making process much more efficient. In this talk, I will cover our work that constructs task-specific latent action space for search-based optimization of black-box functions, finds a representation for policy change that enables joint policy search in imperfect information collaborative games and how different representations affect RL exploration.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_8.html" class="main-title">
                  Evolutionary Algorithms and Game AI
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Simon Lucas  /  Queen Mary University of London
              </span>
                <span class="card-title h3">
                Mar 29, 2021
              </span>
            </div>
            <div class="m-3">
              Evolutionary algorithms are powerful black-box optimisers that find many applications in Game AI. They can be applied in real-time to provide robust policies across a range of games, or at design time for procedural content generation or game parameter tuning. I’ll outline the key concepts, give some insights into why they often work surprisingly well, and discuss future directions.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_7.html" class="main-title">
                  On Question Answering on Images and Databases
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Dzmitry Bahdanau  /  McGill University
              </span>
                <span class="card-title h3">
                Mar 15, 2021
              </span>
            </div>
            <div class="m-3">
              The AI's ability to answer questions that are grounded in context is interesting from both academic and practical perspectives. In this talk, I will present two projects that study question answering (QA) in visual and symbolic database context respectively. In the first project my collaborators and I show that a neuro-symbolic visual QA system can learn variable bindings from the top-down QA training signal only. The system successfully learns both conventional bindings (e.g. objects) and less trivial ones (e.g. groups). In our second project my colleagues at Element AI and I explore text2sql systems for answering questions about databases. We perform extensive ablation testing to understand what is really necessary for such systems to achieve best performance. I will end the talk with a quick preview of our on-going few-shot text2sql research efforts.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_6.html" class="main-title">
                  Learning to Cooperate, Communicate and Coordinate
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Jakob Foerster  /  Facebook AI Research
              </span>
                <span class="card-title h3">
                Mar 08, 2021
              </span>
            </div>
            <div class="m-3">
              In recent years we have seen rapid progress on a number of zero-sum benchmark problems in artificial intelligence, e.g. Go, Poker and Dota. In contrast to these competitive settings, success in the real world typically requires humans, and will require AI agents, to cooperate, communicate and coordinate with others. Crucially, from a learning point of view, these three Cs require fundamentally novel approaches, methods and theory, which has been at the heart of my research agenda. In my talk I will cover recent progress, including how agents can learn to entice others to cooperate in settings of conflicting goals by accounting for their learning behaviour, how they can learn to communicate by reasoning over (public) beliefs and how they can learn policies that can coordinate with other agents at test time by exploiting the symmetries in the environment. I will finish the talk by outlining some of the promising directions for future work.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_5.html" class="main-title">
                  Grounded Language Learning Without Grounded Supervision
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Jacob Andreas  /  MIT
              </span>
                <span class="card-title h3">
                Mar 01, 2021
              </span>
            </div>
            <div class="m-3">
              Central to tasks like instruction following and question answering is the ability to ground linguistic understanding in perception and action. Machine learning models for these tasks typically rely on grounded supervision, e.g. actions paired with human-generated instructions or images paired with human-generated questions and answers. Indeed, several recent papers have argued that general-purpose language understanding (even in text-only tasks like machine reading and text generation) is impossible without grounding. In this talk, I'll present two studies on improving grounded language learning without grounded supervision. First, I'll describe an approach for using multi-agent interaction to fine-tune models for instruction following and instruction generation---without additional human-generated text. Next, I'll describe some very recent work suggesting that language models trained on text alone---without any grounding---are capable of building implicit world models and simulating interactions between entities described in discourse.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_4.html" class="main-title">
                  Increasing generality in reinforcement learning through procedural content generation (or have fun trying)
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Julian Togelius  /  New York University
              </span>
                <span class="card-title h3">
                Jan 18, 2021
              </span>
            </div>
            <div class="m-3">
              Julian Togelius will be giving a talk with the title: "Increasing generality in reinforcement learning through procedural content generation (or have fun trying)", exploring how to make reinforcement learning algorithms overfit to their training environments less, and the use of procedural content generation in reinforcement learning.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_3.html" class="main-title">
                  Intuitive Reasoning as (Un)supervised Neural Generation
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Yejin Choi  /  University of Washington
              </span>
                <span class="card-title h3">
                Jan 04, 2021
              </span>
            </div>
            <div class="m-3">
              Neural language models, as they grow in scale, continue to surprise us with utterly nonsensical and counterintuitive errors despite their otherwise remarkable performances on leaderboards. In this talk, I will argue that it is time to challenge the currently dominant paradigm of task-specific supervision built on top of large-scale self-supervised neural networks. I will first highlight how we can make better lemonade out of neural language models by shifting our focus on unsupervised, inference-time algorithms. I will demonstrate how unsupervised algorithms can match or even outperform supervised approaches on hard reasoning tasks such as nonmonotonic reasoning (such as counterfactual and abductive reasoning), or complex language generation tasks that require logical constraints. Next, I will highlight the importance of melding explicit and declarative knowledge encoded in symbolic knowledge graphs with implicit and observed knowledge encoded in neural language models. I will present COMET, Commonsense Transformers that learn neural representation of commonsense reasoning from a symbolic commonsense knowledge graph, and Social Chemistry 101, a new conceptual formalism, a knowledge graph, and neural models to reason about social, moral, and ethical norms.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_2.html" class="main-title">
                  Social Reinforcement Learning
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Natasha Jaques  /  Google Brain
              </span>
                <span class="card-title h3">
                Dec 21, 2020
              </span>
            </div>
            <div class="m-3">
              Social learning helps humans and animals rapidly adapt to new circumstances, and drives the emergence of complex learned behaviors. This talk focuses on Social Reinforcement Learning, developing new RL algorithms that leverage social learning to improve single-agent learning and generalization, multi-agent coordination, and human-AI interaction. We will demonstrate how a multi-agent technique for Adversarial Environment Generation based on minimax regret can lead to the generation of a complex curriculum of training environments, which improves an agent’s zero-shot transfer to unknown, single-agent test tasks. To improve multi-agent coordination, we give agents an intrinsic motivation to increase their causal influence over the actions of other agents, and show that this leads to the emergence of communication and enhances cooperation. Finally, we propose a novel Offline RL technique for learning from intrinsic social cues during interaction with humans in an open-domain dialog setting. Together, this work argues that Social RL is a valuable approach for developing more general, sophisticated, and human-compatible AI.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="row p-4">
  <div class="col-md-12">
    <div class="card">
      <div class="card-header">
        <div class="row">
          <div class="col-md-2 col-sm-5">
            <img src="" width="100%" class="img-thumbnail" />
          </div>
          <div class="col-md-10 col-sm-6">
            <div class="m-3 text-muted">
              <h3>
                <a href="speaker_1.html" class="main-title">
                  Promise, Progress, and Challenges in Open-Ended Machine Learning
                </a>
              </h3>
            </div>
            <div class="m-3 text-muted">
              <span class="card-title h3">
                Joel Lehman  /  OpenAI
              </span>
                <span class="card-title h3">
                Nov 23, 2020
              </span>
            </div>
            <div class="m-3">
              Researchers in open-ended machine learning are inspired by natural and human processes of innovation (like biological evolution or science itself), and aim to uncover the engineering principles underlying boundlessly creative search algorithms, i.e. algorithms capable of continual production of useful and interesting innovations, The speculative promise for machine learning from such open-ended search includes automated discovery of curricula for reinforcement learning, new neural architectures, and most ambitiously, AGI itself. While its most ambitious potential is far from being realized, conceptual and algorithmic progress has been made. I will review progress in open-ended search and highlight open challenges.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      <div class="border-top my-3"></div>
<div class="row p-4" id="faq">
  <div class="col-12 bd-content">
    <h2 class="text-center">Highlighted Papers</h2>
  </div>
</div>
      <div class="papers">
        <div class="row my-auto mx-auto">
  <div
    id="carouselExample1"
    class="carousel slide d-none d-md-block"
    data-interval="false"
  >
    <div class="carousel-inner cards" role="listbox">
      
    </div>
    <a
      class="carousel-control-prev"
      href="#carouselExample1"
      role="button"
      data-slide="prev"
    >
      <span class="carousel-control-prev-icon" aria-hidden="true"></span>
      <span class="sr-only">Previous</span>
    </a>
    <a
      class="carousel-control-next"
      href="#carouselExample1"
      role="button"
      data-slide="next"
    >
      <span class="carousel-control-next-icon" aria-hidden="true"></span>
      <span class="sr-only">Next</span>
    </a>
  </div>
</div>
      </div>
    </div>

    <script type="text/javascript">
      lazyLoader();
    </script>
  </div>
</div>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2020 UCL DARK Lab</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>