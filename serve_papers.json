[{"UID":"jiang2022tap","abstract":"While planning-based sequence modelling methods have shown great potential in continuous control, scaling them to high-dimensional state-action sequences remains an open challenge due to the high computational complexity and innate difficulty of planning in high-dimensional spaces. We propose the Trajectory Autoencoding Planner (TAP), a planning-based sequence modelling RL method that scales to high state-action dimensionalities. Using a state-conditional Vector-Quantized Variational Autoencoder (VQ-VAE), TAP models the conditional distribution of the trajectories given the current state. When deployed as an RL agent, TAP avoids planning step-by-step in a high-dimensional continuous action space but instead looks for the optimal latent code sequences by beam search. Unlike O(D^3) complexity of Trajectory Transformer, TAP enjoys constant O(C) planning computational complexity regarding state-action dimensionality D. Our empirical evaluation also shows the increasingly strong performance of TAP with the growing dimensionality. For Adroit robotic hand manipulation tasks with high state and action dimensionality, TAP surpasses existing model-based methods, including TT, with a large margin and also beats strong model-free actor-critic baselines.","authors":"Zhengyao Jiang|Tianjun Zhang|Micheal Janner|Yueying Li|Tim Rockt\u00e4schel|Edward Grefenstette|Yuandong Tian","keywords":"reinforcement learning|offline RL|sequence modelling RL|continuous control","proceedings":"arXiv","title":"Efficient Planning in a Compact Latent Action Space","type":"Preprint","url":"https://arxiv.org/abs/2208.10291","year":2022},{"UID":"matthews2022hierarchical","abstract":"Practising and honing skills forms a fundamental component of how humans learn, yet artificial agents are rarely specifically trained to perform them. Instead, they are usually trained end-to-end, with the hope being that useful skills will be implicitly learned in order to maximise discounted return of some extrinsic reward function. In this paper, we investigate how skills can be incorporated into the training of reinforcement learning (RL) agents in complex environments with large state-action spaces and sparse rewards. To this end, we created SkillHack, a benchmark of tasks and associated skills based on the game of NetHack. We evaluate a number of baselines on this benchmark, as well as our own novel skill-based method Hierarchical Kickstarting (HKS), which is shown to outperform all other evaluated methods. Our experiments show that learning with a prior knowledge of useful skills can significantly improve the performance of agents on complex problems. We ultimately argue that utilising predefined skills provides a useful inductive bias for RL problems, especially those with large state-action spaces and sparse rewards.","authors":"Michael Matthews|Mikayel Samvelyan|Jack Parker-Holder|Edward Grefenstette|Tim Rockt\u00e4schel","keywords":"reinforcement learning|transfer learning|environment","proceedings":"CoLLAs","title":"Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning","type":"Conference","url":"https://arxiv.org/pdf/2207.11584.pdf","year":2022},{"UID":"jiang2022gb","abstract":"The successes of deep Reinforcement Learning (RL) are limited to settings where we have a large stream of online experiences, but applying RL in the data-efficient setting with limited access to online interactions is still challenging. A key to data-efficient RL is good value estimation, but current methods in this space fail to fully utilise the structure of the trajectory data gathered from the environment. In this paper, we treat the transition data of the MDP as a graph, and define a novel backup operator, Graph Backup, which exploits this graph structure for better value estimation. Compared to multi-step backup methods such as n-step Q-Learning and TD(\u03bb), Graph Backup can perform counterfactual credit assignment and gives stable value estimates for a state regardless of which trajectory the state is sampled from. Our method, when combined with popular value-based methods, provides improved performance over one-step and multi-step methods on a suite of data-efficient RL benchmarks including MiniGrid, Minatar and Atari100K. We further analyse the reasons for this performance boost through a novel visualisation of the transition graphs of Atari games.","authors":"Zhengyao Jiang|Tianjun Zhang|Robert Kirk|Tim Rockt\u00e4schel|Edward Grefenstette","keywords":"reinforcement learning|graph structure|data-efficient RL","proceedings":"arXiv","title":"Graph Backup: Data Efficient Backup Exploiting Markovian Transitions","type":"Preprint","url":"https://arxiv.org/abs/2205.15824","year":2022},{"UID":"parker-holder2022evolving","abstract":"It remains a significant challenge to train generally capable agents with reinforcement learning (RL). A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agent's capabilities. These methods benefit from their generality, with theoretical guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces. By contrast, evolutionary approaches seek to incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. In this paper we propose to harness the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Levels (ACCEL), seeks to constantly produce levels at the frontier of an agent's capabilities, resulting in curricula that start simple but become increasingly complex. ACCEL maintains the theoretical benefits of prior regret-based methods, while providing significant empirical gains in a diverse set of environments. An interactive version of the paper is available at accelagent.github.io.","authors":"Jack Parker-Holder|Minqi Jiang|Michael Dennis|Mikayel Samvelyan|Jakob Foerster|Edward Grefenstette|Tim Rockt\u00e4schel","keywords":"reinforcement learning|generalisation|open-endedness|environment design","proceedings":"ICML","title":"Evolving Curricula with Regret-Based Environment Design","type":"Conference","url":"https://arxiv.org/abs/2203.01302","year":2022},{"UID":"mahajan2022generalization","abstract":"Collective intelligence is a fundamental trait shared by several species of living organisms. It has allowed them to thrive in the diverse environmental conditions that exist on our planet. From simple organisations in an ant colony to complex systems in human groups, collective intelligence is vital for solving complex survival tasks. As is commonly observed, such natural systems are flexible to changes in their structure. Specifically, they exhibit a high degree of generalization when the abilities or the total number of agents changes within a system. We term this phenomenon as Combinatorial Generalization (CG). CG is a highly desirable trait for autonomous systems as it can increase their utility and deployability across a wide range of applications. While recent works addressing specific aspects of CG have shown impressive results on complex domains, they provide no performance guarantees when generalizing towards novel situations. In this work, we shed light on the theoretical underpinnings of CG for cooperative multi-agent systems (MAS). Specifically, we study generalization bounds under a linear dependence of the underlying dynamics on the agent capabilities, which can be seen as a generalization of Successor Features to MAS. We then extend the results first for Lipschitz and then arbitrary dependence of rewards on team capabilities. Finally, empirical analysis on various domains using the framework of multi-agent reinforcement learning highlights important desiderata for multi-agent algorithms towards ensuring CG.","authors":"Anuj Mahajan|Mikayel Samvelyan|Tarun Gupta|Benjamin Ellis|Mingfei Sun|Tim Rockt\u00e4schel|Shimon Whiteson","keywords":"reinforcement learning|multi-agent|generalisation","proceedings":"arXiv","title":"Generalization in Cooperative Multi-Agent Systems","type":"Preprint","url":"https://arxiv.org/abs/2202.00104","year":2022},{"UID":"kirk2021survey","abstract":"The study of generalisation in deep Reinforcement Learning (RL) aims to produce RL algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overfitting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent field. We provide a unifying formalism and terminology for discussing different generalisation problems, building upon previous works. We go on to categorise existing benchmarks for generalisation, as well as current methods for tackling the generalisation problem. Finally, we provide a critical discussion of the current state of the field, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in generalisation, we suggest fast online adaptation and tackling RL-specific problems as some areas for future work on methods for generalisation, and we recommend building benchmarks in underexplored problem settings such as offline RL generalisation and reward-function variation.","authors":"Robert Kirk|Amy Zhang|Edward Grefenstette|Tim Rockt\u00e4schel","blog":"generalization_survey.md","keywords":"reinforcement learning|generalisation|survey|review","proceedings":"arXiv","title":"A Survey of Generalisation in Deep Reinforcement Learning","type":"Preprint","url":"https://arxiv.org/abs/2111.09794","year":2021},{"UID":"samvelyan2021minihack","abstract":"The progress in deep reinforcement learning (RL) is heavily driven by the availability of challenging benchmarks used for training agents. However, benchmarks that are widely adopted by the community are not explicitly designed for evaluating specific capabilities of RL methods. While there exist environments for assessing particular open problems in RL (such as exploration, transfer learning, unsupervised environment design, or even language-assisted RL), it is generally difficult to extend these to richer, more complex environments once research goes beyond proof-of-concept results. We present MiniHack, a powerful sandbox framework for easily designing novel RL environments. MiniHack is a one-stop shop for RL experiments with environments ranging from small rooms to complex, procedurally generated worlds. By leveraging the full set of entities and environment dynamics from NetHack, one of the richest grid-based video games, MiniHack allows designing custom RL testbeds that are fast and convenient to use. With this sandbox framework, novel environments can be designed easily, either using a human-readable description language or a simple Python interface. In addition to a variety of RL tasks and baselines, MiniHack can wrap existing RL benchmarks and provide ways to seamlessly add additional complexity.","authors":"Mikayel Samvelyan|Robert Kirk|Vitaly Kurin|Jack Parker-Holder|Minqi Jiang|Eric Hambro|Fabio Petroni|Heinrich K\u00fcttler|Edward Grefenstette|Tim Rockt\u00e4schel","keywords":"reinforcement learning|open-endedness|environment design|environment","proceedings":"NeurIPS","title":"MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research","type":"Conference","url":"https://arxiv.org/abs/2109.13202","year":2021},{"UID":"jiang2021replay","abstract":"Deep reinforcement learning (RL) agents may successfully generalize to new settings if trained on an appropriately diverse set of environment and task configurations. Unsupervised Environment Design (UED) is a promising self-supervised RL paradigm, wherein the free parameters of an underspecified environment are automatically adapted during training to the agent's capabilities, leading to the emergence of diverse training environments. Here, we cast Prioritized Level Replay (PLR), an empirically successful but theoretically unmotivated method that selectively samples randomly-generated training levels, as UED. We argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). Crucially, DCD includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. This connection allows us to develop novel theory for PLR, providing a version with a robustness guarantee at Nash equilibria. Furthermore, our theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), we can improve the convergence to Nash equilibria. Indeed, our experiments confirm that our new method, PLR\u22a5, obtains better results on a suite of out-of-distribution, zero-shot transfer tasks, in addition to demonstrating that PLR\u22a5 improves the performance of PAIRED, from which it inherited its theoretical framework.","authors":"Minqi Jiang|Michael Dennis|Jack Parker-Holder|Jakob Foerster|Edward Grefenstette|Tim Rockt\u00e4schel","keywords":"reinforcement learning|generalisation|curriculum learning|environment design","proceedings":"NeurIPS","title":"Replay-Guided Adversarial Environment Design","type":"Conference","url":"https://arxiv.org/abs/2110.02439","year":2021},{"UID":"jiang2020prioritized","abstract":"Simulated environments with procedurally generated content have become popular benchmarks for testing systematic generalization of reinforcement learning agents. Every level in such an environment is algorithmically created, thereby exhibiting a unique configuration of underlying factors of variation, such as layout, positions of entities, asset appearances, or even the rules governing environment transitions. Fixed sets of training levels can be determined to aid comparison and reproducibility, and test levels can be held out to evaluate the generalization and robustness of agents. While prior work samples training levels in a direct way (e.g. uniformly) for the agent to learn from, we investigate the hypothesis that different levels provide different learning progress for an agent at specific times during training. We introduce Prioritized Level Replay, a general framework for estimating the future learning potential of a level given the current state of the agent's policy. We find that temporal-difference (TD) errors, while previously used to selectively sample past transitions, also prove effective for scoring a level's future learning potential when the agent replays (that is, revisits) that level to generate entirely new episodes of experiences from it. We report significantly improved sample-efficiency and generalization on the majority of Procgen Benchmark environments as well as two challenging MiniGrid environments. Lastly, we present a qualitative analysis showing that Prioritized Level Replay induces an implicit curriculum, taking the agent gradually from easier to harder levels","authors":"Minqi Jiang|Edward Grefenstette|Tim Rockt\u00e4schel","keywords":"reinforcement learning|curriculum learning|generalisation|procedural content generation","proceedings":"ICML","title":"Prioritized Level Replay","type":"Conference","url":"https://arxiv.org/abs/2010.03934","year":2021},{"UID":"jiang2021gtg","abstract":"Although reinforcement learning has been successfully applied in many domains in recent years, we still lack agents that can systematically generalize. While relational inductive biases that fit a task can improve generalization of RL agents, these biases are commonly hard-coded directly in the agent's neural architecture. In this work, we show that we can incorporate relational inductive biases, encoded in the form of relational graphs, into agents. Based on this insight, we propose Grid-to-Graph (GTG), a mapping from grid structures to relational graphs that carry useful spatial relational inductive biases when processed through a Relational Graph Convolution Network (R-GCN). We show that, with GTG, R-GCNs generalize better both in terms of in-distribution and out-of-distribution compared to baselines based on Convolutional Neural Networks and Neural Logic Machines on challenging procedurally generated environments and MinAtar. Furthermore, we show that GTG produces agents that can jointly reason over observations and environment dynamics encoded in knowledge bases.","authors":"Zhengyao Jiang|Pasquale Minervini|Minqi Jiang|Tim Rocktaschel","keywords":"Relational Inductive Bias|Reinforcement Learning|Graph Neural Network","proceedings":"AAMAS","title":"Grid-to-Graph: Flexible Spatial Relational Inductive Biases for Reinforcement Learning","type":"Conference","url":"https://arxiv.org/abs/2102.04220","year":2021},{"UID":"mahajan2021tesseract","abstract":"Reinforcement Learning in large action spaces is a challenging problem. Cooperative multi-agent reinforcement learning (MARL) exacerbates matters by imposing various constraints on communication and observability. In this work, we consider the fundamental hurdle affecting both value-based and policy-gradient approaches: an exponential blowup of the action space with the number of agents. For value-based methods, it poses challenges in accurately representing the optimal value function. For policy gradient methods, it makes training the critic difficult and exacerbates the problem of the lagging critic. We show that from a learning theory perspective, both problems can be addressed by accurately representing the associated action-value function with a low-complexity hypothesis class. This requires accurately modelling the agent interactions in a sample efficient way. To this end, we propose a novel tensorised formulation of the Bellman equation. This gives rise to our method Tesseract, which views the Q-function as a tensor whose modes correspond to the action spaces of different agents. Algorithms derived from Tesseract decompose the Q-tensor across agents and utilise low-rank tensor approximations to model agent interactions relevant to the task. We provide PAC analysis for Tesseract-based algorithms and highlight their relevance to the class of rich observation MDPs. Empirical results in different domains confirm Tesseract's gains in sample efficiency predicted by the theory.","authors":"Anuj Mahajan|Mikayel Samvelyan|Lei Mao|Viktor Makoviychuk|Animesh Garg|Jean Kossaifi|Shimon Whiteson|Yuke Zhu|Animashree Anandkumar","keywords":"reinforcement learning|multi-agent learning","proceedings":"ICML","title":"Tesseract: Tensorised Actors for Multi-Agent Reinforcement Learning","type":"Conference","url":"https://arxiv.org/abs/2106.00136","year":2021},{"UID":"niepert2021imle","abstract":"Combining discrete probability distributions and combinatorial optimization problems with neural network components has numerous applications but poses several challenges. We propose Implicit Maximum Likelihood Estimation (I-MLE), a framework for end-to-end learning of models combining discrete exponential family distributions and differentiable neural components. I-MLE is widely applicable as it only requires the ability to compute the most probable states and does not rely on smooth relaxations. The framework encompasses several approaches such as perturbation-based implicit differentiation and recent methods to differentiate through black-box combinatorial solvers. We introduce a novel class of noise distributions for approximating marginals via perturb-and-MAP. Moreover, we show that I-MLE simplifies to maximum likelihood estimation when used in some recently studied learning settings that involve combinatorial solvers. Experiments on several datasets suggest that I-MLE is competitive with and often outperforms existing approaches which rely on problem-specific relaxations.","authors":"Mathias Niepert|Pasquale Minervini|Luca Franceschi","keywords":"reasoning|planning|gradient estimation|discrete distributions|backpropagation","proceedings":"NeurIPS","title":"Implicit MLE: Backpropagating Through Discrete Exponential Family Distribution","type":"Conference","url":"https://arxiv.org/abs/2106.01798","year":2021},{"UID":"campero2021amigo","abstract":"A key challenge for reinforcement learning (RL) consists of learning in environments with sparse extrinsic rewards. In contrast to current RL methods, humans are able to learn new skills with little or no reward by using various forms of intrinsic motivation. We propose AMIGo, a novel agent incorporating a goal-generating teacher that proposes Adversarially Motivated Intrinsic Goals to train a goal-conditioned 'student' policy in the absence of (or alongside) environment reward. Specifically, through a simple but effective 'constructively adversarial' objective, the teacher learns to propose increasingly challenging\u2014yet achievable\u2014goals that allow the student to learn general skills for acting in a new environment, independent of the task to be solved. We show that our method generates a natural curriculum of self-proposed goals which ultimately allows the agent to solve challenging procedurally-generated tasks where other forms of intrinsic motivation and state-of-the-art RL methods fail.","authors":"Andres Campero|Roberta Raileanu|Heinrich K\u00fcttler|Joshua B. Tenenbaum|Tim Rockt\u00e4schel|Edward Grefenstette","keywords":"exploration|reinforcement learning","proceedings":"ICLR","title":"Learning with AMIGo: Adversarially Motivated Intrinsic Goals","type":"Conference","url":"https://arxiv.org/abs/2006.12122","year":2021},{"UID":"arakelyan2021cqd","abstract":"Neural link predictors are immensely useful for identifying missing edges in large scale Knowledge Graphs. However, it is still not clear how to use these models for answering more complex queries that arise in a number of domains, such as queries using logical conjunctions (\u2227), disjunctions (\u2228) and existential quantifiers (\u2203), while accounting for missing edges. In this work, we propose a framework for efficiently answering complex queries on incomplete Knowledge Graphs. We translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. We then analyse two solutions to the optimisation problem, including gradient-based and combinatorial search. In our experiments, the proposed approach produces more accurate results than state-of-the-art methods -- black-box neural models trained on millions of generated queries -- without the need of training on a large and diverse set of complex queries. Using orders of magnitude less training data, we obtain relative improvements ranging from 8% up to 40% in Hits@3 across different knowledge graphs containing factual information. Finally, we demonstrate that it is possible to explain the outcome of our model in terms of the intermediate solutions identified for each of the complex query atoms. All our source code and datasets are available online.","authors":"Erik Arakelyan|Daniel Daza|Pasquale Minervini|Michael Cochez","keywords":"complex query answering|Knowledge Graphs","proceedings":"ICLR","title":"Complex Query Answering with Neural Link Predictors","type":"Conference","url":"https://arxiv.org/abs/2011.03459","year":"2021 (Outstanding Paper Award)"},{"UID":"kuettler2020nethack","abstract":"Progress in Reinforcement Learning (RL) algorithms goes hand-in-hand with the development of challenging environments that test the limits of current methods. While existing RL environments are either sufficiently complex or based on fast simulation, they are rarely both. Here, we present the NetHack Learning Environment (NLE), a scalable, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular single-player terminal-based roguelike game, NetHack. We argue that NetHack is sufficiently complex to drive long-term research on problems such as exploration, planning, skill acquisition, and language-conditioned RL, while dramatically reducing the computational resources required to gather a large amount of experience. We compare NLE and its task suite to existing alternatives, and discuss why it is an ideal medium for testing the robustness and systematic generalization of RL agents. We demonstrate empirical success for early stages of the game using a distributed Deep RL baseline and Random Network Distillation exploration, alongside qualitative analysis of various agents trained in the environment. NLE is open source at https://github.com/facebookresearch/nle.","authors":"Heinrich K\u00fcttler|Nantas Nardelli|Alexander H. Miller|Roberta Raileanu|Marco Selvatici|Edward Grefenstette|Tim Rockt\u00e4schel","keywords":"environment|reinforcement learning","proceedings":"NeurIPS","title":"The NetHack Learning Environment","type":"Conference","url":"https://arxiv.org/abs/2006.13760","year":2020},{"UID":"jiang2020wordcraft","abstract":"The ability to quickly solve a wide range of real-world tasks requires a commonsense understanding of the world. Yet, how to best extract such knowledge from natural language corpora and integrate it with reinforcement learning (RL) agents remains an open challenge. This is partly due to the lack of lightweight simulation environments that sufficiently reflect the semantics of the real world and provide knowledge sources grounded with respect to observations in an RL environment. To better enable research on agents making use of commonsense knowledge, we propose WordCraft, an RL environment based on Little Alchemy 2. This lightweight environment is fast to run and built upon entities and relations inspired by real-world semantics. We evaluate several representation learning methods on this new benchmark and propose a new method for integrating knowledge graphs with an RL agent.","authors":"Minqi Jiang|Jelena Luketina|Nantas Nardelli|Pasquale Minervini|Philip H.S. Torr|Shimon Whiteson|Tim Rockt\u00e4schel","keywords":"reinforcement learning|commonsense reasoning|natural language processing|procedural content generation","proceedings":"Workshop on Language in Reinforcement Learning at ICML","title":"WordCraft: An Environment for Benchmarking Commonsense Agents","type":"Conference","url":"https://arxiv.org/abs/2007.09185","year":2020},{"UID":"raileanu2020ride","abstract":"Exploration in sparse reward environments remains one of the key challenges of model-free reinforcement learning. Instead of solely relying on extrinsic rewards provided by the environment, many state-of-the-art methods use intrinsic rewards to encourage exploration. However, we show that existing methods fall short in procedurally-generated environments where an agent is unlikely to visit a state more than once. We propose a novel type of intrinsic reward which encourages the agent to take actions that lead to significant changes in its learned state representation. We evaluate our method on multiple challenging procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. Our experiments demonstrate that this approach is more sample efficient than existing exploration methods, particularly for procedurally-generated MiniGrid environments. Furthermore, we analyze the learned behavior as well as the intrinsic reward received by our agent. In contrast to previous approaches, our intrinsic reward does not diminish during the course of training and it rewards the agent substantially more for interacting with objects that it can control.","authors":"Roberta Raileanu|Tim Rockt\u00e4schel","keywords":"exploration|reinforcement learning","proceedings":"ICLR","title":"RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments","type":"Conference","url":"https://arxiv.org/abs/2002.12292.abs","year":2020},{"UID":"zhong2020rtfm","abstract":"Obtaining policies that can generalise to new environments in reinforcement learning is challenging. In this work, we demonstrate that language understanding via a reading policy learner is a promising vehicle for generalisation to new environments. We propose a grounded policy learning problem, Read to Fight Monsters (RTFM), in which the agent must jointly reason over a language goal, relevant dynamics described in a document, and environment observations. We procedurally generate environment dynamics and corresponding language descriptions of the dynamics, such that agents must read to understand new environment dynamics instead of memorising any particular information. In addition, we propose txt2\u03c0, a model that captures three-way interactions between the goal, document, and observations. On RTFM, txt2\u03c0 generalises to new environments with dynamics not seen during training via reading. Furthermore, our model outperforms baselines such as FiLM and language-conditioned CNNs on RTFM. Through curriculum learning, txt2\u03c0 produces policies that excel on complex RTFM tasks requiring several reasoning and coreference steps.","authors":"Victor Zhong|Tim Rockt\u00e4schel|Edward Grefenstette","keywords":"natural language processing|reasoning|reinforcement learning","proceedings":"ICLR","title":"RTFM: Generalising to New Environment Dynamics via Reading","type":"Conference","url":"https://arxiv.org/abs/1910.08210","year":2020}]
