[{"UID":"23","abstract":"","bio":"","date":"Aug 31, 2022","institution":"Deepmind","speaker":"Wojciech Czarnecki","title":"On the Geometry of Competitive Games","video":"https://www.youtube.com/embed/RGH97toWiCk"},{"UID":"22","abstract":"As interest in the field of open-endedness expands, ideas like continual discovery and increasingly complexity have gained significant attention. The aim of this talk is to bring attention to two lesser-known facets of open-endedness that nevertheless merit significantly more attention. First, evidence so far suggests that representations achieved through open-ended processes seem radically different from representations discovered through optimization, even if they solve the same problem. The implications of this observation are unknown but potentially significant. Second, while the agents themselves in an open-ended system are of course critical, in processes analogous to civilization (as opposed to evolution), the artifacts those agents put into and leave in the environment (e.g. houses and cars for humans) are also critical to the trajectory of the system, yet little studied or understood. New advances, such as Evolution through Large Models, make studying this issue more feasible. The hope is that this talk will inspire further investigation into both considerations.","bio":"Kenneth O. Stanley is currently deciding his next adventure after most recently leading a research team at OpenAI on the challenge of open-endedness. He was previously Charles Millican Professor of Computer Science at the University of Central Florida and was also a co-founder of Geometric Intelligence Inc., which was acquired by Uber to create Uber AI Labs, where he was head of Core AI research. He received a B.S.E. from the University of Pennsylvania and received a Ph.D. from the University of Texas at Austin. He is an inventor of the Neuroevolution of Augmenting Topologies (NEAT), HyperNEAT, novelty search, POET, and ELM algorithms, as well as the CPPN representation, among many others. His main research contributions are in neuroevolution (i.e. evolving neural networks), generative and developmental systems, coevolution, machine learning for video games, interactive evolution, quality diversity, and open-endedness. He has won best paper awards for his work on NEAT, NERO, NEAT Drummer, FSMC, HyperNEAT, novelty search, Galactic Arms Race, POET, and MCC. His original 2002 paper on NEAT also received the 2017 ISAL Award for Outstanding Paper of the Decade 2002 - 2012 from the International Society for Artificial Life. He is a coauthor of the popular science book, \"Why Greatness Cannot Be Planned: The Myth of the Objective\" (published by Springer), and has spoken widely on its subject.","date":"Aug 24, 2022","institution":"Meta AI","speaker":"Franziska Meier","title":"Novel Opportunities in Open-Endedness","video":"https://www.youtube.com/embed/T08wc4xD3KA"},{"UID":"21","abstract":"While there has been major investment in developing robot learning algorithms, achieving true autonomy remains a wide-open research question. A key limitation of current learning approaches is the assumption that learning is a one-time event on a pre-defined task-distribution and that an expert can manually specify how a robot should learn these tasks from multi-modal sensor data. This approach to setting up the learning problem fundamentally hinders robots from being able to learn new tasks or adapt to new situations autonomously. In this talk, I will discuss the challenges of the frontier of a lifelong learning robot and what algorithmic advancements are required to advance the state-of-the-art in autonomous robotics. In short, progress towards lifelong learning robots requires algorithms that enable a robot to learn new skills incrementally and continuously (without forgetting), autonomously (without expert intervention) and sample-efficiently. In this context, I will present our recent advances towards autonomous learning of robotic manipulation skills by enabling the robot to learn reward functions while only requiring minimal expert intervention. Specifically, I will present a unified framework for model-based and model-free reward learning algorithms, that can either learn rewards from demonstrations or under-specified (sparse) rewards.  Finally, I will discuss the challenge of learning rewards that generalize to novel settings, and present initial insights from our empirical analysis of how well learned rewards generalize.","bio":"Franziska Meier is a research scientist at FAIR (Facebook AI Research). Previously she was a research scientist at the Max-Planck Institute for Intelligent Systems and a postdoctoral researcher with Dieter Fox at the University of Washington, Seattle. She received her PhD from the University of Southern California, where she defended her thesis on \u201cProbabilistic Machine Learning for Robotics\u201d in 2016, under the supervision of Prof. Stefan Schaal. Prior to her PhD studies, she received her Diploma in Computer Science from the Technical University of Munich. Her research focuses on machine learning for robotics, with a special emphasis on lifelong learning for robotics.","date":"March 24, 2022","institution":"Meta AI","speaker":"Franziska Meier","title":"Lifelong Learning for Robotics","video":"https://www.youtube.com/embed/3SVT-Y10Zno"},{"UID":"20","abstract":"The capabilities of modern machine learning systems are to a large extent determined by their ability to effectively utilize large and diverse datasets. However, such systems typically focus on making predictions rather than making decisions, with the aim of maximizing the likelihood of some data rather than a user-specified utility function. Reinforcement learning methods directly address the problem of utility maximization, but such methods are difficult to reconcile with modern data-driven learning, and typically require either active data collection or specially tailored datasets, both of which are not conducive for being able to use large datasets. In this talk, I will discuss how learning-based control can be performed with offline data, and how such offline RL algorithms can utilize comparatively less specialized datasets with general-purpose objectives to enable learning to make decisions at scale. I will discuss the algorithmic foundations of offline reinforcement learning, and present a number of robotics applications that use comparatively general data sources in the areas of robotic navigation and manipulation.","bio":"Sergey Levine received a BS and MS in Computer Science from Stanford University in 2009, and a Ph.D. in Computer Science from Stanford University in 2014. He joined the faculty of the Department of Electrical Engineering and Computer Sciences at UC Berkeley in fall 2016. His work focuses on machine learning for decision making and control, with an emphasis on deep learning and reinforcement learning algorithms. Applications of his work include autonomous robots and vehicles, as well as computer vision and graphics. His research includes developing algorithms for end-to-end training of deep neural network policies that combine perception and control, scalable algorithms for inverse reinforcement learning, deep reinforcement learning algorithms, and more. His work has been featured in many popular press outlets, including the New York Times, the BBC, MIT Technology Review, and Bloomberg Business.","date":"January 6, 2022","institution":"UC Berkeley","speaker":"Sergey Levine","title":"Understanding the World Through Action","video":"https://www.youtube.com/embed/yXImQEMS77g"},{"UID":"19","abstract":"The world around us \u2014 and our understanding of it \u2014 is rich in compositional structure: from atoms and their interactions to objects and entities in our environments. How can we learn models of the world that take this structure into account and generalize to new compositions in systematic ways? This talk focuses on an emerging class of slot-based neural architectures that can discover, represent, and reason about abstract entities from perceptual input alone. Taking our recent work on Slot Attention as an example, I will explain the challenges for object discovery and how attention-based routing provides an elegant solution to mapping from low-level perceptual features to high-level object-centric abstractions. With Slot Attention for Video (SAVi), we extend this framework to temporally-consistent modeling of objects over time and show how information about object motion can help the model find the right decomposition of a scene into its constituent components. Finally, I will discuss some of the open challenges that remain for developing and deploying structured world models.","bio":"Thomas Kipf is a Research Scientist at Google Brain in Amsterdam. His research focuses on developing machine learning models that can reason about the rich structure of the physical world, using structured abstractions such as objects, entities, and their relations. He obtained his PhD from the University of Amsterdam with a thesis on \u201cDeep Learning with Graph-Structured Representations\u201d, supervised by Max Welling. His work received a best paper award at ESWC2018 and he was recently elected as an ELLIS Scholar in \u201cSemantic, Symbolic and Interpretable Machine Learning\u201d.","date":"December 16, 2021","institution":"Google Brain","speaker":"Thomas Kipf","title":"Learning Structured Models of the World","video":"https://youtube.com/embed/oLKwRBeBRRA"},{"UID":"18","abstract":"In this talk I will cover our recent publication \"Open-Ended Learning Leads to Generally Capable Agents\". In this work we turn our attention to how to create embodied agents in simulation that can generalise to unseen test tasks and exhibit generally capable behaviour. I will introduce our XLand procedurally generated environment, and the open-ended learning algorithms that allow us to train agents to cover this vast environment space. This results in agents that are capable across a wide range of held-out test tasks including hide-and-seek and capture-the-flag, and we will explore these results and the emergent behaviours and representations of the agent.","bio":"Max is a research scientist at DeepMind, where he leads the Open-Ended Learning research team. Known in the past for awesome work on XLand and StarCraft. He co-founded Vision Factory which was acquired by Google in 2014, and completed his PhD at the Visual Geometry Group at the University of Oxford under the supervision of Prof. Andrew Zisserman and Prof. Andrea Vedaldi.","date":"November 25, 2021","institution":"DeepMind","speaker":"Max Jaderberg","title":"Open-Ended Learning Leads to Generally Capable Agents","video":"https://youtube.com/embed/UiZYXoOxmvU"},{"UID":"17","abstract":"Recently there has been a shift in computer vision research from static vision, e.g., object detection, to Embodied AI, e.g., robot navigation. In this talk, I will focus on the task of PointGoal navigation (PointNav) where an embodied agent (virtual robot) is tasked with navigating to a point specified relative to its initial location in an unknown environment and without a map. I will ask and answer: Given only a depth camera and localization sensor, is this task learnable with generic tools, model-free reinforcement learning (RL) and generic neural networks? To answer this question, my collaborators and I developed a new distributed system for RL designed to meet the needs of training in realistic simulation. We showed that this task is entirely learnable by training an agent for the equivalent of 80 years of human experience. We then designed a new simulation paradigm specifically for the needs of RL centered on large batch simulation, where the simulator simulates many agents in many environments at once and is responsible for its own parallelization, reducing training wall-clock time from 6 GPU months to 36 GPU hours, an over 100x improvement.","bio":"Erik is a PhD student at Georgia Institute of Technology, advised by Irfan Essa and Dhruv Batra. His work is primarily focused on computer vision and its applications to artificial intelligence, with the long-term research goal of developing fundamental techniques, algorithms, and large-scale systems for robotic assistants. This work focuses on sim2real transfer and embodied AI. He's worked on the AI Habitat platform for Embodied AI, as well as being the lead organiser for two Embodied AI workshops at CVPR (2020, 2021).","date":"September 2, 2021","institution":"Georgia Institute of Technology","speaker":"Erik Wijmans","title":"Training Virtual Robots in Realistic Simulators","video":"https://www.youtube.com/embed/plgA5gmj8ZQ"},{"UID":"16","abstract":"Developing agents capable of learning complex human-like behaviors is a key goal of artificial intelligence research. Progress towards this goal has exciting potential for applications in video games, from new tools that empower game developers to realize new creative visions, to enabling new kinds of immersive player experiences. In this talk I will share insights recently developed by my team and myself on how reinforcement learning, and other AI techniques can give rise to more human-like bot or NPC behaviors, focusing on the following components. First, I motivate the need for accurately evaluating human-likeness, propose a solution within a single-agent navigation task, and show that achieving highly skilled behavior is insufficient for human-likeness. Second, I demonstrate the first agent that passes our bar for human-likeness. Finally, I discuss how to go beyond single agent tasks and towards learning to collaborate, using structured models for predicting other agents\u2019 behavior.","bio":"Katja is a Principal Researcher and lead of Game Intelligence at Microsoft Research Cambridge. Her research focuses on reinforcement learning, driven by current and future applications in video games, with the long-term goal of developing AI systems that learn to collaborate with people, to empower their users and help solve complex real-world problems. Her team is behind Project Malmo, a sophisticated AI experimentation platform built on top of Minecraft. She completed her PhD in Computer Science as part of the Information and Language Processing Systems group at the University of Amsterdam, supervised by Maarten de Rijke and Shimon Whiteson.","date":"August 19, 2021","institution":"Microsoft Research","speaker":"Katja Hofmann","title":"Towards Human-Like And Collaborative AI in Video Games","video":"https://www.youtube.com/embed/64Dxl1z633Q"},{"UID":"15","abstract":"Learning agents in text-based games are faced with the challenge of understanding and generating language to accomplish various goals. Decomposing this primary challenge, I will highlight recent work to address sub-challenges of knowledge representation, affordance detection, commonsense reasoning, and language grounding. Specifically, I will discuss knowledge graphs as a means of tracking agent state, language models as generators for valid actions, and ALFWorld, a new environment that aligns text and embodied modalities for the study of language grounding.","bio":"Matthew is a Senior Researcher in the Reinforcement Learning Group at Microsoft Research, Redmond. He previously obtained his PhD in Computer Science from the University of Texas at Austin advised by Professor Peter Stone. He's worked on a variety of topics within reinforcement learning and machine learning, including introducing deep recurrent Q-networks and working on the Arcade Learning Environment. Most recently his research has focused on text-based games and agents.","date":"July 8, 2021","institution":"Microsoft Research","speaker":"Matthew Hausknecht","title":"Towards Knowledge Grounded Text Agents","video":"https://www.youtube.com/embed/jhIPJH1ghhg"},{"UID":"14","abstract":"Video games have become an attractive testbed for evaluating AI systems, by capturing some aspects of real-world complexity (rich visual stimuli and non-trivial decision policies) while abstracting away from other sources of complexity (e.g., sensory transduction and motor planning). Some AI researchers have reported human-level performance of their systems, but we still have very little insight into how humans actually learn to play video games. This talk will present new data on human video game learning indicating that humans learn very differently from most current AI systems, particularly those based on deep learning. Humans can induce object-oriented, relational models from a small amount of experience, which allow them to learn quickly, explore intelligently, plan efficiently, and generalize flexibly. These aspects of human-like learning can be captured by a model that learns through a form of program induction.","bio":"Sam is an Associate Professor in the Department of Psychology and Center for Brain Science at Harvard, where he leads the Computational Cognitive Neuroscience Lab. His research focuses on computational cognitive neuroscience approaches to learning, memory and decision making. He received my B.A. in Neuroscience and Behavior from Columbia University in 2007 and his Ph.D. in Psychology and Neuroscience from Princeton University in 2013, where he worked with Ken Norman and Yael Niv. From 2013-2015 he was a postdoctoral fellow in the Department of Brain and Cognitive Sciences at MIT, working with Josh Tenenbaum and Nancy Kanwisher.","date":"June 21, 2021","institution":"Harvard University","speaker":"Sam Gershman","title":"Using Video Games To Reverse Engineer Human Intelligence","video":"https://www.youtube.com/embed/vIAOuj-7gRM"},{"UID":"13","abstract":"The benefit of multi-task learning over single-task learning relies on the ability to use relations across tasks to improve performance on any single task. While sharing representations is an important mechanism to share information across tasks, its success depends on how well the structure underlying the tasks is captured. In some real-world situations, we have access to metadata, or additional information about a task, that may not provide any new insight in the context of a single task setup alone but inform relations across multiple tasks. While this metadata can be useful for improving multi-task learning performance, effectively incorporating it can be an additional challenge. In this talk, we explore various ways to utilize context to improve positive transfer in multi-task reinforcement learning.","bio":"Amy is a postdoctoral scholar at UC Berkeley and a research scientist at Facebook AI Research. She works on state abstractions, model-based reinforcement learning, representation learning, and generalization in RL. Amy completed her PhD at McGill University and Mila - Quebec AI Institute, co-supervised by Joelle Pineau and Doina Precup. She has an M.Eng. in EECS and dual B.Sci. degrees in Mathematics and EECS from MIT.","date":"June 7, 2021","institution":"UC Berkeley and Facebook AI Research","speaker":"Amy Zhang","title":"Exploring Context for Better Generalization in Reinforcement Learning","video":"https://www.youtube.com/embed/akeUVn6WQoU"},{"UID":"12","abstract":"In this talk, I'll give four good reasons to do embodied machine learning. Learning in an agent that can perceive and interact with its environment is fundamentally different from other ML settings. Unlike a disembodied model, an embodied learner must learn to perceive and move in addition to mastering whatever specific behaviour the user may be interested in. This may seem like a disadvantage, because there is in some sense more to learn, but it can also be an advantage when trying to replicate human cognitive behaviours like reasoning and generalization. Having a body and being necessarily located at a specific place at a given time places strong constraints on the learner's experience, which in turn leads to more human-like learning outcomes. These results suggest that embodied learning may be an important part of what is needed to convincingly replicate human linguistic intuitions and behaviours in a machine.","bio":"Felix Hill is a Research Scientist at DeepMind, where he works on replicating in artificial systems how we as humans learn and use natural language. His work is at the intersection of natural language processing & understanding and reinforcement learning, particularly focusing on embodied language learning, taking inspiration from cognitive science and psychology. He completed his PhD in computational linguistic at Cambridge University in 2016 in the Natural Language Information and Processing Group, supervised by Anna Korhonen. During his PhD he also spent time working at the LISA lab, Montreal, with Yoshua Bengio.","date":"May 24, 2021","institution":"DeepMind","speaker":"Felix Hill","title":"Some Reasons To Do Embodied Machine Learning","video":"https://www.youtube.com/embed/dYwEK_WjYuM"},{"UID":"11","abstract":"Mental simulation\u2014the capacity to imagine what will or what could be\u2014is a salient feature of human cognition, playing a key role in a wide range of cognitive abilities. In artificial intelligence, the last few years have seen the development of model-based deep reinforcement learning methods, which seemingly share many similarities with mental simulation. In this talk, I will discuss how closely such methods actually capture the qualitative characteristics exhibited by human mental simulation, with a particular focus on: (1) the extent to which the performance of such agents is driven by model-based reasoning and planning, and (2) how effectively such agents can leverage planning for generalization. While a number of challenges remain in matching the capacity of human mental simulation, I will highlight some recent progress on developing more compositional model-based algorithms through the use of graph neural networks and tree search.","bio":"Jessica Hamrick is a Senior Research Scientist at DeepMind, where she studies how to build machines that can flexibly build and deploy models of the world. Her work combines insights from cognitive science with structured relational architectures, model-based deep reinforcement learning, and planning. She completed my PhD in Psychology at UC Berkeley in Tom Griffiths\u2019 Computational Cognitive Science Lab. Before that, she researched intuitive physics in Josh Tenenbaum\u2019s Computational Cognitive Science Group at MIT.","date":"May 10, 2021","institution":"DeepMind","speaker":"Jessica Hamrick","title":"Mental Simulation, Imagination, and\u2028Model-Based Deep RL","video":"https://www.youtube.com/embed/et9ivpY0Kyw"},{"UID":"10","abstract":"The ability to cooperate through language is a defining feature of humans. As the perceptual, motory and planning capabilities of deep artificial networks increase, researchers are studying whether they can also develop a shared language to interact. In this talk, I will highlight recent advances in this field but also common headaches (or perhaps limitations) with respect to experimental setup and evaluation of emergent communication. Towards making multi-agent communication a building block of human-centric AI, and by drawing from my own recent work, I will discuss approaches on making emergent communication relevant for human-agent communication in natural language.","bio":"Angeliki Lazaridou is a research scientist at DeepMind. Before that, she was a graduate student of Marco Baroni working on grounded language learning at the CLIC Lab of the Center for Mind/Brain Sciences of the University of Trento, Italy. Before that, she did an MSc in Computational Linguistics at the University of Saarland working with Ivan Titov and Caroline Sporleder on Sentiment Analysis, supported by an Erasmus Mundus Masters scholarship in Language and Communication Technology (EM-LCT).","date":"Apr 26, 2021","institution":"DeepMind","speaker":"Angeliki Lazaridou","title":"Towards multi-agent emergent communication as a building block of human-centric AI","video":null},{"UID":"9","abstract":"How to learn good latent representations is an important topic in the modern era of machine learning. For reinforcement learning, using a good representation makes the decision-making process much more efficient. In this talk, I will cover our work that constructs task-specific latent action space for search-based optimization of black-box functions, finds a representation for policy change that enables joint policy search in imperfect information collaborative games and how different representations affect RL exploration.","bio":"Yuandong Tian is a Research Scientist and Manager in Facebook AI Research, working on deep reinforcement learning and representation learning. He is the lead scientist and engineer for ELF OpenGo and DarkForest Go projects. Prior to that, he was in Google Self-driving Car team in 2013-2014. He received a Ph.D in Robotics Institute, Carnegie Mellon University in 2013. He is the recipient of 2013 ICCV Marr Prize Honorable Mentions.","date":"Apr 12, 2021","institution":"Facebook AI Research","speaker":"Yuandong Tian","title":"Finding Good Representation for Search and Exploration in RL","video":"https://www.youtube.com/embed/sH4a2a0ntUA"},{"UID":"8","abstract":"Evolutionary algorithms are powerful black-box optimisers that find many applications in Game AI. They can be applied in real-time to provide robust policies across a range of games, or at design time for procedural content generation or game parameter tuning. I\u2019ll outline the key concepts, give some insights into why they often work surprisingly well, and discuss future directions.","bio":"Simon Lucas is a professor of Artificial Intelligence and Head of the School of Electronic Engineering and Computer Science at Queen Mary University of London where he also heads the Game AI Research Group. He holds a PhD degree (1991) in Electronics and Computer Science from the University of Southampton. He is the founding Editor-in-Chief of the IEEE Transactions on Games and co-founded the IEEE Conference on Conference on Games. His research involves simulation-based AI and evolutionary algorithms applied to Game AI, and work towards Artificial General Intelligence. He is a fellow of the Alan Turing Institute, and a visiting researcher at Facebook","date":"Mar 29, 2021","institution":"Queen Mary University of London","speaker":"Simon Lucas","title":"Evolutionary Algorithms and Game AI","video":"https://www.youtube.com/embed/MM3JoMYcAYQ"},{"UID":"7","abstract":"The AI's ability to answer questions that are grounded in context is interesting from both academic and practical perspectives. In this talk, I will present two projects that study question answering (QA) in visual and symbolic database context respectively. In the first project my collaborators and I show that a neuro-symbolic visual QA system can learn variable bindings from the top-down QA training signal only. The system successfully learns both conventional bindings (e.g. objects) and less trivial ones (e.g. groups). In our second project my colleagues at Element AI and I explore text2sql systems for answering questions about databases. We perform extensive ablation testing to understand what is really necessary for such systems to achieve best performance. I will end the talk with a quick preview of our on-going few-shot text2sql research efforts.","bio":"Dzmitry Bahdanau is an Adjunct Professor at McGill University and a research scientist at ServiceNow Element AI. Prior to that, he obtained his PhD at Mila and Universit\u00e9 de Montr\u00e9al working with Yoshua Bengio. He is interested in fundamental and applied questions concerning natural language understanding. His main research areas include semantic parsing, language user interfaces, systematic generalization and hybrid neural-symbolic systems. He invented the content-based neural attention that is now a core tool in deep-learning-based natural language processing.","date":"Mar 15, 2021","institution":"McGill University","speaker":"Dzmitry Bahdanau","title":"On Question Answering on Images and Databases","video":null},{"UID":"6","abstract":"In recent years we have seen rapid progress on a number of zero-sum benchmark problems in artificial intelligence, e.g. Go, Poker and Dota. In contrast to these competitive settings, success in the real world typically requires humans, and will require AI agents, to cooperate, communicate and coordinate with others. Crucially, from a learning point of view, these three Cs require fundamentally novel approaches, methods and theory, which has been at the heart of my research agenda. In my talk I will cover recent progress, including how agents can learn to entice others to cooperate in settings of conflicting goals by accounting for their learning behaviour, how they can learn to communicate by reasoning over (public) beliefs and how they can learn policies that can coordinate with other agents at test time by exploiting the symmetries in the environment. I will finish the talk by outlining some of the promising directions for future work.","bio":"Jakob Foerster is a research scientist at Facebook AI Research, incoming assistant professor at the University of Toronto and a Canada CIFAR AI Chair. His work focuses on multi-agent reinforcement learning, emergent communication, human-AI coordination, game theory & planning. He earned his PhD under Shimon Whiteson at the University of Oxford, where he helped bring deep multi-agent reinforcement learning to the forefront of AI research and interned at Google Brain, OpenAI and DeepMind. He was the lead organiser of the first Emergent Communication (EmeCom) workshop at NeurIPS in 2017, which he has helped organise ever since.","date":"Mar 08, 2021","institution":"Facebook AI Research","speaker":"Jakob Foerster","title":"Learning to Cooperate, Communicate and Coordinate","video":"https://www.youtube.com/embed/TMTT2z8lifA"},{"UID":"5","abstract":"Central to tasks like instruction following and question answering is the ability to ground linguistic understanding in perception and action. Machine learning models for these tasks typically rely on grounded supervision, e.g. actions paired with human-generated instructions or images paired with human-generated questions and answers. Indeed, several recent papers have argued that general-purpose language understanding (even in text-only tasks like machine reading and text generation) is impossible without grounding. In this talk, I'll present two studies on improving grounded language learning without grounded supervision. First, I'll describe an approach for using multi-agent interaction to fine-tune models for instruction following and instruction generation---without additional human-generated text. Next, I'll describe some very recent work suggesting that language models trained on text alone---without any grounding---are capable of building implicit world models and simulating interactions between entities described in discourse.","bio":"Jacob Andreas is the X Consortium Assistant Professor at MIT. His research focuses on building intelligent systems that can communicate effectively using language and learn from human guidance. Jacob earned his Ph.D. from UC Berkeley, his M.Phil. from Cambridge (where he studied as a Churchill scholar) and his B.S. from Columbia. He has been the recipient of an NSF graduate fellowship, a Facebook fellowship, and paper awards at NAACL and ICML.","date":"Mar 01, 2021","institution":"MIT","speaker":"Jacob Andreas","title":"Grounded Language Learning Without Grounded Supervision","video":"https://www.youtube.com/embed/el7CvuYLvIA"},{"UID":"4","abstract":"Julian Togelius will be giving a talk with the title: \"Increasing generality in reinforcement learning through procedural content generation (or have fun trying)\", exploring how to make reinforcement learning algorithms overfit to their training environments less, and the use of procedural content generation in reinforcement learning.","bio":"Julian Togelius is an associate professor at the Department of Computer Science and Engineering at the New York University Tandon School of Engineering, and the co-founder of modl.ai. He's done seminal research in procedural content generation for games and artificial intelligence, including co-authoring the textbook Artificial Intelligence and Games. Previously, he was an associate professor at the Center for Computer Games Research, IT University of Copenhagen. He holds a BA from Lund University, an MSc from the University of Sussex, and a PhD from the University of Essex.","date":"Jan 18, 2021","institution":"New York University","speaker":"Julian Togelius","title":"Increasing generality in reinforcement learning through procedural content generation (or have fun trying)","video":"https://www.youtube.com/embed/9KPcUgnjpMg"},{"UID":"3","abstract":"Neural language models, as they grow in scale, continue to surprise us with utterly nonsensical and counterintuitive errors despite their otherwise remarkable performances on leaderboards. In this talk, I will argue that it is time to challenge the currently dominant paradigm of task-specific supervision built on top of large-scale self-supervised neural networks. I will first highlight how we can make better lemonade out of neural language models by shifting our focus on unsupervised, inference-time algorithms. I will demonstrate how unsupervised algorithms can match or even outperform supervised approaches on hard reasoning tasks such as nonmonotonic reasoning (such as counterfactual and abductive reasoning), or complex language generation tasks that require logical constraints. Next, I will highlight the importance of melding explicit and declarative knowledge encoded in symbolic knowledge graphs with implicit and observed knowledge encoded in neural language models. I will present COMET, Commonsense Transformers that learn neural representation of commonsense reasoning from a symbolic commonsense knowledge graph, and Social Chemistry 101, a new conceptual formalism, a knowledge graph, and neural models to reason about social, moral, and ethical norms.","bio":"Yejin Choi is a Brett Helsel associate professor at the Paul G. Allen School of Computer Science & Engineering at the University of Washington and also a senior research manager at AI2 overseeing the project Mosaic. Her research interests include commonsense knowledge and reasoning, neural language (de-)generation, language grounding, and AI for social good. She is a co-recipient of the AAAI Outstanding Paper Award in 2020, Borg Early Career Award (BECA) in 2018, IEEE\u2019s AI Top 10 to Watch in 2015, the ICCV Marr Prize in 2013, and the inaugural Alexa Prize Challenge in 2017.","date":"Jan 04, 2021","institution":"University of Washington","speaker":"Yejin Choi","title":"Intuitive Reasoning as (Un)supervised Neural Generation","video":"https://www.youtube.com/embed/YITTv93xHXU"},{"UID":"2","abstract":"Social learning helps humans and animals rapidly adapt to new circumstances, and drives the emergence of complex learned behaviors. This talk focuses on Social Reinforcement Learning, developing new RL algorithms that leverage social learning to improve single-agent learning and generalization, multi-agent coordination, and human-AI interaction. We will demonstrate how a multi-agent technique for Adversarial Environment Generation based on minimax regret can lead to the generation of a complex curriculum of training environments, which improves an agent\u2019s zero-shot transfer to unknown, single-agent test tasks. To improve multi-agent coordination, we give agents an intrinsic motivation to increase their causal influence over the actions of other agents, and show that this leads to the emergence of communication and enhances cooperation. Finally, we propose a novel Offline RL technique for learning from intrinsic social cues during interaction with humans in an open-domain dialog setting. Together, this work argues that Social RL is a valuable approach for developing more general, sophisticated, and human-compatible AI.","bio":"Natasha Jaques holds a joint position as a Research Scientist at Google Brain and post-doc at UC Berkeley. Her research focuses on social reinforcement learning---developing multi-agent RL algorithms that can improve single-agent learning, generalization, coordination, and human-AI collaboration. Natasha received her PhD from MIT, where she focused on Affective Computing and new techniques for deep/reinforcement/machine learning. Her work has received the best demo award at NeurIPS 2016, best paper at the NeurIPS ML for Healthcare workshop, and an honourable mention for best paper at ICML 2019. She has interned at DeepMind, Google Brain, and is an OpenAI Scholars mentor. Her work has been featured in Quartz, the MIT Technology Review, Boston Magazine, and on CBC radio.  Natasha earned her Masters degree from the University of British Columbia, and undergraduate degrees in Computer Science and Psychology from the University of Regina","date":"Dec 21, 2020","institution":"Google Brain","speaker":"Natasha Jaques","title":"Social Reinforcement Learning","video":"https://www.youtube.com/embed/traKBhJm4lQ"},{"UID":"1","abstract":"Researchers in open-ended machine learning are inspired by natural and human processes of innovation (like biological evolution or science itself), and aim to uncover the engineering principles underlying boundlessly creative search algorithms, i.e. algorithms capable of continual production of useful and interesting innovations, The speculative promise for machine learning from such open-ended search includes automated discovery of curricula for reinforcement learning, new neural architectures, and most ambitiously, AGI itself. While its most ambitious potential is far from being realized, conceptual and algorithmic progress has been made. I will review progress in open-ended search and highlight open challenges.","bio":"Joel Lehman is a research scientist at OpenAI, and previously was a founding member of Uber AI Labs and an assistant professor at the IT University of Copenhagen. His research focuses on open-endedness, reinforcement learning, evolutionary computation, and AI safety. His PhD dissertation introduced novelty search, an influential method within evolutionary computation, which inspired a popular science book co-written with Ken Stanley on what search algorithms imply for individual and societal objectives, called \u201cWhy Greatness Cannot Be Planned.\u201d","date":"Nov 23, 2020","institution":"OpenAI","speaker":"Joel Lehman","title":"Promise, Progress, and Challenges in Open-Ended Machine Learning","video":null}]
