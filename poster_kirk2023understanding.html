


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="stylesheet" href="static/css/main.css" type="text/css"/>
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="icon" href="static/images/logo.png">

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
    <script src="https://kit.fontawesome.com/c59ce62110.js" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />

    <title>UCL DARK Lab: Understanding the Effects of RLHF on LLM Generalisation and Diversity</title>
    
<meta name="citation_title" content="Understanding the Effects of RLHF on LLM Generalisation and Diversity" />

<meta name="citation_author" content="Robert Kirk" />

<meta name="citation_author" content="Ishita Mediratta" />

<meta name="citation_author" content="Christoforos Nalmpantis" />

<meta name="citation_author" content="Jelena Luketina" />

<meta name="citation_author" content="Eric Hambro" />

<meta name="citation_author" content="Edward Grefenstette" />

<meta name="citation_author" content="Roberta Raileanu" />

<meta name="citation_publication_date" content="None" />
<meta name="citation_conference_title" content="Ucl Deciding, Acting, And Reasoning With Knowledge (Dark) Lab" />
<meta name="citation_inbook_title" content="None" />
<meta name="citation_abstract" content="Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI&#39;s ChatGPT or Anthropic&#39;s Claude. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties&#34;:&#34; out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model&#39;s ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the tradeoff between generalisation and diversity." />

<meta name="citation_keywords" content="large language models" />

<meta name="citation_keywords" content="rlhf" />

<meta name="citation_keywords" content="generalisation" />

<meta name="citation_keywords" content="diversity" />

<meta name="citation_pdf_url" content="https://arxiv.org/abs/2310.06452" />


  </head>

  <body>
    <!-- NAV -->
    
    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto"
      id="main-nav"
    >
        <div class="container">
        <!--
        <a class="navbar-brand" href="index.html">
          <img
             class="logo" src="static/images/logo.png"
             height="auto"
             width="130px"
          />
        </a>
        -->
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="index.html">Home</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Publications</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="speakers.html">Speakers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="https://blog.ucldark.com/">Blog</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      Understanding the Effects of RLHF on LLM Generalisation and Diversity
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Robert Kirk" class="text-muted"
        >Robert Kirk</a
      >,
      
      <a href="papers.html?filter=authors&search=Ishita Mediratta" class="text-muted"
        >Ishita Mediratta</a
      >,
      
      <a href="papers.html?filter=authors&search=Christoforos Nalmpantis" class="text-muted"
        >Christoforos Nalmpantis</a
      >,
      
      <a href="papers.html?filter=authors&search=Jelena Luketina" class="text-muted"
        >Jelena Luketina</a
      >,
      
      <a href="papers.html?filter=authors&search=Eric Hambro" class="text-muted"
        >Eric Hambro</a
      >,
      
      <a href="papers.html?filter=authors&search=Edward Grefenstette" class="text-muted"
        >Edward Grefenstette</a
      >,
      
      <a href="papers.html?filter=authors&search=Roberta Raileanu" class="text-muted"
        >Roberta Raileanu</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Keywords:</span>
      
      <a
        href="papers.html?filter=keywords&search=large language models"
        class="text-secondary text-decoration-none"
        >large language models</a
      >,
      
      <a
        href="papers.html?filter=keywords&search=rlhf"
        class="text-secondary text-decoration-none"
        >rlhf</a
      >,
      
      <a
        href="papers.html?filter=keywords&search=generalisation"
        class="text-secondary text-decoration-none"
        >generalisation</a
      >,
      
      <a
        href="papers.html?filter=keywords&search=diversity"
        class="text-secondary text-decoration-none"
        >diversity</a
      >
      
    </p>
    <div class="text-center p-3">
        
      <a class="card-link" data-toggle="collapse" role="button" href="#details">
        Abstract
      </a>
        
      <a class="card-link" target="_blank" href="https://arxiv.org/abs/2310.06452">
        Paper
      </a>
      
    </div>
  </div>
</div>

    <div id="details" class="pp-card m-3">
      <div class="card-body">
        <div class="card-text">
          <div id="abstractExample">
            <span class="font-weight-bold">Abstract:</span>
            Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI&#39;s ChatGPT or Anthropic&#39;s Claude. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties&#34;:&#34; out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model&#39;s ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the tradeoff between generalisation and diversity.
          </div>
        </div>
        <p></p>
      </div>
    </div>



      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2020 UCL DARK Lab</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>