- title: 'Using Video Games To Reverse Engineer Human Intelligence'
  speaker: Sam Gershman
  institution: Harvard University
  date: June 21, 2021
  abstract: "Video games have become an attractive testbed for evaluating AI systems, by capturing some aspects of real-world complexity (rich visual stimuli and non-trivial decision policies) while abstracting away from other sources of complexity (e.g., sensory transduction and motor planning). Some AI researchers have reported human-level performance of their systems, but we still have very little insight into how humans actually learn to play video games. This talk will present new data on human video game learning indicating that humans learn very differently from most current AI systems, particularly those based on deep learning. Humans can induce object-oriented, relational models from a small amount of experience, which allow them to learn quickly, explore intelligently, plan efficiently, and generalize flexibly. These aspects of human-like learning can be captured by a model that learns through a form of program induction."
  bio: "Sam is an Associate Professor in the Department of Psychology and Center for Brain Science at Harvard, where he leads the Computational Cognitive Neuroscience Lab. His research focuses on computational cognitive neuroscience approaches to learning, memory and decision making. He received my B.A. in Neuroscience and Behavior from Columbia University in 2007 and his Ph.D. in Psychology and Neuroscience from Princeton University in 2013, where he worked with Ken Norman and Yael Niv. From 2013-2015 he was a postdoctoral fellow in the Department of Brain and Cognitive Sciences at MIT, working with Josh Tenenbaum and Nancy Kanwisher."
  video: https://www.youtube.com/embed/vIAOuj-7gRM
  UID: "14"
- title: 'Exploring Context for Better Generalization in Reinforcement Learning'
  speaker: Amy Zhang
  institution: UC Berkeley and Facebook AI Research
  date: June 7, 2021
  abstract: "The benefit of multi-task learning over single-task learning relies on the ability to use relations across tasks to improve performance on any single task. While sharing representations is an important mechanism to share information across tasks, its success depends on how well the structure underlying the tasks is captured. In some real-world situations, we have access to metadata, or additional information about a task, that may not provide any new insight in the context of a single task setup alone but inform relations across multiple tasks. While this metadata can be useful for improving multi-task learning performance, effectively incorporating it can be an additional challenge. In this talk, we explore various ways to utilize context to improve positive transfer in multi-task reinforcement learning."
  bio: "Amy is a postdoctoral scholar at UC Berkeley and a research scientist at Facebook AI Research. She works on state abstractions, model-based reinforcement learning, representation learning, and generalization in RL. Amy completed her PhD at McGill University and Mila - Quebec AI Institute, co-supervised by Joelle Pineau and Doina Precup. She has an M.Eng. in EECS and dual B.Sci. degrees in Mathematics and EECS from MIT."
  video: https://www.youtube.com/embed/akeUVn6WQoU
  UID: "13"
- title: 'Some Reasons To Do Embodied Machine Learning'
  speaker: Felix Hill
  institution: DeepMind
  date: May 24, 2021
  abstract: "In this talk, I'll give four good reasons to do embodied machine learning. Learning in an agent that can perceive and interact with its environment is fundamentally different from other ML settings. Unlike a disembodied model, an embodied learner must learn to perceive and move in addition to mastering whatever specific behaviour the user may be interested in. This may seem like a disadvantage, because there is in some sense more to learn, but it can also be an advantage when trying to replicate human cognitive behaviours like reasoning and generalization. Having a body and being necessarily located at a specific place at a given time places strong constraints on the learner's experience, which in turn leads to more human-like learning outcomes. These results suggest that embodied learning may be an important part of what is needed to convincingly replicate human linguistic intuitions and behaviours in a machine."
  bio: Felix Hill is a Research Scientist at DeepMind, where he works on replicating in artificial systems how we as humans learn and use natural language. His work is at the intersection of natural language processing & understanding and reinforcement learning, particularly focusing on embodied language learning, taking inspiration from cognitive science and psychology. He completed his PhD in computational linguistic at Cambridge University in 2016 in the Natural Language Information and Processing Group, supervised by Anna Korhonen. During his PhD he also spent time working at the LISA lab, Montreal, with Yoshua Bengio.
  video: https://www.youtube.com/embed/dYwEK_WjYuM
  UID: "12"
- title: 'Mental Simulation, Imagination, and  Model-Based Deep RL'
  speaker: Jessica Hamrick
  institution: DeepMind
  date: May 10, 2021
  abstract: 'Mental simulation—the capacity to imagine what will or what could be—is a salient feature of human cognition, playing a key role in a wide range of cognitive abilities. In artificial intelligence, the last few years have seen the development of model-based deep reinforcement learning methods, which seemingly share many similarities with mental simulation. In this talk, I will discuss how closely such methods actually capture the qualitative characteristics exhibited by human mental simulation, with a particular focus on: (1) the extent to which the performance of such agents is driven by model-based reasoning and planning, and (2) how effectively such agents can leverage planning for generalization. While a number of challenges remain in matching the capacity of human mental simulation, I will highlight some recent progress on developing more compositional model-based algorithms through the use of graph neural networks and tree search.'
  bio: Jessica Hamrick is a Senior Research Scientist at DeepMind, where she studies how to build machines that can flexibly build and deploy models of the world. Her work combines insights from cognitive science with structured relational architectures, model-based deep reinforcement learning, and planning. She completed my PhD in Psychology at UC Berkeley in Tom Griffiths’ Computational Cognitive Science Lab. Before that, she researched intuitive physics in Josh Tenenbaum’s Computational Cognitive Science Group at MIT.
  video: https://www.youtube.com/embed/et9ivpY0Kyw
  UID: "11"
- title: 'Towards multi-agent emergent communication as a building block of human-centric AI'
  speaker: Angeliki Lazaridou
  institution: DeepMind
  date: Apr 26, 2021
  abstract: The ability to cooperate through language is a defining feature of humans. As the perceptual, motory and planning capabilities of deep artificial networks increase, researchers are studying whether they can also develop a shared language to interact. In this talk, I will highlight recent advances in this field but also common headaches (or perhaps limitations) with respect to experimental setup and evaluation of emergent communication. Towards making multi-agent communication a building block of human-centric AI, and by drawing from my own recent work, I will discuss approaches on making emergent communication relevant for human-agent communication in natural language.
  bio: Angeliki Lazaridou is a research scientist at DeepMind. Before that, she was a graduate student of Marco Baroni working on grounded language learning at the CLIC Lab of the Center for Mind/Brain Sciences of the University of Trento, Italy. Before that, she did an MSc in Computational Linguistics at the University of Saarland working with Ivan Titov and Caroline Sporleder on Sentiment Analysis, supported by an Erasmus Mundus Masters scholarship in Language and Communication Technology (EM-LCT).
  video:
  UID: "10"
- title: Finding Good Representation for Search and Exploration in RL
  speaker: Yuandong Tian
  institution: Facebook AI Research
  date: Apr 12, 2021
  abstract: How to learn good latent representations is an important topic in the modern era of machine learning. For reinforcement learning, using a good representation makes the decision-making process much more efficient. In this talk, I will cover our work that constructs task-specific latent action space for search-based optimization of black-box functions, finds a representation for policy change that enables joint policy search in imperfect information collaborative games and how different representations affect RL exploration.
  bio: Yuandong Tian is a Research Scientist and Manager in Facebook AI Research, working on deep reinforcement learning and representation learning. He is the lead scientist and engineer for ELF OpenGo and DarkForest Go projects. Prior to that, he was in Google Self-driving Car team in 2013-2014. He received a Ph.D in Robotics Institute, Carnegie Mellon University in 2013. He is the recipient of 2013 ICCV Marr Prize Honorable Mentions.
  video: https://www.youtube.com/embed/sH4a2a0ntUA
  UID: "9"
- title: Evolutionary Algorithms and Game AI
  speaker: Simon Lucas
  institution: Queen Mary University of London
  date: Mar 29, 2021
  abstract: Evolutionary algorithms are powerful black-box optimisers that find many applications in Game AI. They can be applied in real-time to provide robust policies across a range of games, or at design time for procedural content generation or game parameter tuning. I’ll outline the key concepts, give some insights into why they often work surprisingly well, and discuss future directions.
  bio: Simon Lucas is a professor of Artificial Intelligence and Head of the School of Electronic Engineering and Computer Science at Queen Mary University of London where he also heads the Game AI Research Group. He holds a PhD degree (1991) in Electronics and Computer Science from the University of Southampton. He is the founding Editor-in-Chief of the IEEE Transactions on Games and co-founded the IEEE Conference on Conference on Games. His research involves simulation-based AI and evolutionary algorithms applied to Game AI, and work towards Artificial General Intelligence. He is a fellow of the Alan Turing Institute, and a visiting researcher at Facebook
  video: https://www.youtube.com/embed/MM3JoMYcAYQ
  UID: "8"
- title: On Question Answering on Images and Databases
  speaker: Dzmitry Bahdanau
  institution: McGill University
  date: Mar 15, 2021
  abstract: The AI's ability to answer questions that are grounded in context is interesting from both academic and practical perspectives. In this talk, I will present two projects that study question answering (QA) in visual and symbolic database context respectively. In the first project my collaborators and I show that a neuro-symbolic visual QA system can learn variable bindings from the top-down QA training signal only. The system successfully learns both conventional bindings (e.g. objects) and less trivial ones (e.g. groups). In our second project my colleagues at Element AI and I explore text2sql systems for answering questions about databases. We perform extensive ablation testing to understand what is really necessary for such systems to achieve best performance. I will end the talk with a quick preview of our on-going few-shot text2sql research efforts.
  bio: Dzmitry Bahdanau is an Adjunct Professor at McGill University and a research scientist at ServiceNow Element AI. Prior to that, he obtained his PhD at Mila and Université de Montréal working with Yoshua Bengio. He is interested in fundamental and applied questions concerning natural language understanding. His main research areas include semantic parsing, language user interfaces, systematic generalization and hybrid neural-symbolic systems. He invented the content-based neural attention that is now a core tool in deep-learning-based natural language processing.
  video:
  UID: "7"
- title: Learning to Cooperate, Communicate and Coordinate
  speaker: Jakob Foerster
  institution: Facebook AI Research
  date: Mar 08, 2021
  abstract: In recent years we have seen rapid progress on a number of zero-sum benchmark problems in artificial intelligence, e.g. Go, Poker and Dota. In contrast to these competitive settings, success in the real world typically requires humans, and will require AI agents, to cooperate, communicate and coordinate with others. Crucially, from a learning point of view, these three Cs require fundamentally novel approaches, methods and theory, which has been at the heart of my research agenda. In my talk I will cover recent progress, including how agents can learn to entice others to cooperate in settings of conflicting goals by accounting for their learning behaviour, how they can learn to communicate by reasoning over (public) beliefs and how they can learn policies that can coordinate with other agents at test time by exploiting the symmetries in the environment. I will finish the talk by outlining some of the promising directions for future work.
  bio: Jakob Foerster is a research scientist at Facebook AI Research, incoming assistant professor at the University of Toronto and a Canada CIFAR AI Chair. His work focuses on multi-agent reinforcement learning, emergent communication, human-AI coordination, game theory & planning. He earned his PhD under Shimon Whiteson at the University of Oxford, where he helped bring deep multi-agent reinforcement learning to the forefront of AI research and interned at Google Brain, OpenAI and DeepMind. He was the lead organiser of the first Emergent Communication (EmeCom) workshop at NeurIPS in 2017, which he has helped organise ever since.
  video: https://www.youtube.com/embed/TMTT2z8lifA
  UID: "6"
- title: Grounded Language Learning Without Grounded Supervision
  speaker: Jacob Andreas
  institution: MIT
  date: Mar 01, 2021
  abstract: Central to tasks like instruction following and question answering is the ability to ground linguistic understanding in perception and action. Machine learning models for these tasks typically rely on grounded supervision, e.g. actions paired with human-generated instructions or images paired with human-generated questions and answers. Indeed, several recent papers have argued that general-purpose language understanding (even in text-only tasks like machine reading and text generation) is impossible without grounding. In this talk, I'll present two studies on improving grounded language learning without grounded supervision. First, I'll describe an approach for using multi-agent interaction to fine-tune models for instruction following and instruction generation---without additional human-generated text. Next, I'll describe some very recent work suggesting that language models trained on text alone---without any grounding---are capable of building implicit world models and simulating interactions between entities described in discourse.
  bio: Jacob Andreas is the X Consortium Assistant Professor at MIT. His research focuses on building intelligent systems that can communicate effectively using language and learn from human guidance. Jacob earned his Ph.D. from UC Berkeley, his M.Phil. from Cambridge (where he studied as a Churchill scholar) and his B.S. from Columbia. He has been the recipient of an NSF graduate fellowship, a Facebook fellowship, and paper awards at NAACL and ICML.
  video: https://www.youtube.com/embed/el7CvuYLvIA
  UID: "5"
- title: Increasing generality in reinforcement learning through procedural content generation (or have fun trying)
  speaker: Julian Togelius
  institution: New York University
  date: Jan 18, 2021
  abstract: 'Julian Togelius will be giving a talk with the title: "Increasing generality in reinforcement learning through procedural content generation (or have fun trying)", exploring how to make reinforcement learning algorithms overfit to their training environments less, and the use of procedural content generation in reinforcement learning.'
  bio: Julian Togelius is an associate professor at the Department of Computer Science and Engineering at the New York University Tandon School of Engineering, and the co-founder of modl.ai. He's done seminal research in procedural content generation for games and artificial intelligence, including co-authoring the textbook Artificial Intelligence and Games. Previously, he was an associate professor at the Center for Computer Games Research, IT University of Copenhagen. He holds a BA from Lund University, an MSc from the University of Sussex, and a PhD from the University of Essex.
  video: https://www.youtube.com/embed/9KPcUgnjpMg
  UID: "4"
- title: Intuitive Reasoning as (Un)supervised Neural Generation
  speaker: Yejin Choi
  institution: University of Washington
  date: Jan 04, 2021
  abstract: Neural language models, as they grow in scale, continue to surprise us with utterly nonsensical and counterintuitive errors despite their otherwise remarkable performances on leaderboards. In this talk, I will argue that it is time to challenge the currently dominant paradigm of task-specific supervision built on top of large-scale self-supervised neural networks. I will first highlight how we can make better lemonade out of neural language models by shifting our focus on unsupervised, inference-time algorithms. I will demonstrate how unsupervised algorithms can match or even outperform supervised approaches on hard reasoning tasks such as nonmonotonic reasoning (such as counterfactual and abductive reasoning), or complex language generation tasks that require logical constraints. Next, I will highlight the importance of melding explicit and declarative knowledge encoded in symbolic knowledge graphs with implicit and observed knowledge encoded in neural language models. I will present COMET, Commonsense Transformers that learn neural representation of commonsense reasoning from a symbolic commonsense knowledge graph, and Social Chemistry 101, a new conceptual formalism, a knowledge graph, and neural models to reason about social, moral, and ethical norms.
  bio: Yejin Choi is a Brett Helsel associate professor at the Paul G. Allen School of Computer Science & Engineering at the University of Washington and also a senior research manager at AI2 overseeing the project Mosaic. Her research interests include commonsense knowledge and reasoning, neural language (de-)generation, language grounding, and AI for social good. She is a co-recipient of the AAAI Outstanding Paper Award in 2020, Borg Early Career Award (BECA) in 2018, IEEE’s AI Top 10 to Watch in 2015, the ICCV Marr Prize in 2013, and the inaugural Alexa Prize Challenge in 2017.
  video: https://www.youtube.com/embed/YITTv93xHXU
  UID: "3"
- title: Social Reinforcement Learning
  speaker: Natasha Jaques
  institution: Google Brain
  date: Dec 21, 2020
  abstract: Social learning helps humans and animals rapidly adapt to new circumstances, and drives the emergence of complex learned behaviors. This talk focuses on Social Reinforcement Learning, developing new RL algorithms that leverage social learning to improve single-agent learning and generalization, multi-agent coordination, and human-AI interaction. We will demonstrate how a multi-agent technique for Adversarial Environment Generation based on minimax regret can lead to the generation of a complex curriculum of training environments, which improves an agent’s zero-shot transfer to unknown, single-agent test tasks. To improve multi-agent coordination, we give agents an intrinsic motivation to increase their causal influence over the actions of other agents, and show that this leads to the emergence of communication and enhances cooperation. Finally, we propose a novel Offline RL technique for learning from intrinsic social cues during interaction with humans in an open-domain dialog setting. Together, this work argues that Social RL is a valuable approach for developing more general, sophisticated, and human-compatible AI.
  bio: Natasha Jaques holds a joint position as a Research Scientist at Google Brain and post-doc at UC Berkeley. Her research focuses on social reinforcement learning---developing multi-agent RL algorithms that can improve single-agent learning, generalization, coordination, and human-AI collaboration. Natasha received her PhD from MIT, where she focused on Affective Computing and new techniques for deep/reinforcement/machine learning. Her work has received the best demo award at NeurIPS 2016, best paper at the NeurIPS ML for Healthcare workshop, and an honourable mention for best paper at ICML 2019. She has interned at DeepMind, Google Brain, and is an OpenAI Scholars mentor. Her work has been featured in Quartz, the MIT Technology Review, Boston Magazine, and on CBC radio.  Natasha earned her Masters degree from the University of British Columbia, and undergraduate degrees in Computer Science and Psychology from the University of Regina
  video: https://www.youtube.com/embed/traKBhJm4lQ
  UID: "2"
- title: Promise, Progress, and Challenges in Open-Ended Machine Learning
  speaker: Joel Lehman
  institution: OpenAI
  date: Nov 23, 2020
  abstract: Researchers in open-ended machine learning are inspired by natural and human processes of innovation (like biological evolution or science itself), and aim to uncover the engineering principles underlying boundlessly creative search algorithms, i.e. algorithms capable of continual production of useful and interesting innovations, The speculative promise for machine learning from such open-ended search includes automated discovery of curricula for reinforcement learning, new neural architectures, and most ambitiously, AGI itself. While its most ambitious potential is far from being realized, conceptual and algorithmic progress has been made. I will review progress in open-ended search and highlight open challenges.
  bio: Joel Lehman is a research scientist at OpenAI, and previously was a founding member of Uber AI Labs and an assistant professor at the IT University of Copenhagen. His research focuses on open-endedness, reinforcement learning, evolutionary computation, and AI safety. His PhD dissertation introduced novelty search, an influential method within evolutionary computation, which inspired a popular science book co-written with Ken Stanley on what search algorithms imply for individual and societal objectives, called “Why Greatness Cannot Be Planned.”
  video:
  UID: "1"
