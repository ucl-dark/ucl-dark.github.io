- title: Increasing generality in reinforcement learning through procedural content generation (or have fun trying)
  speaker: Julian Togelius
  institution: New York University
  date: Jan 18, 2021
  abstract: 'Julian Togelius will be giving a talk with the title: "Increasing generality in reinforcement learning through procedural content generation (or have fun trying)", exploring how to make reinforcement learning algorithms overfit to their training environments less, and the use of procedural content generation in reinforcement learning.'
  bio: Julian Togelius is an associate professor at the Department of Computer Science and Engineering at the New York University Tandon School of Engineering, and the co-founder of modl.ai. He's done seminal research in procedural content generation for games and artificial intelligence, including co-authoring the textbook Artificial Intelligence and Games. Previously, he was an associate professor at the Center for Computer Games Research, IT University of Copenhagen. He holds a BA from Lund University, an MSc from the University of Sussex, and a PhD from the University of Essex.
  video: https://www.youtube.com/embed/9KPcUgnjpMg
  UID: "4"
- title: Intuitive Reasoning as (Un)supervised Neural Generation
  speaker: Yejin Choi
  institution: University of Washington
  date: Jan 04, 2021
  abstract: Neural language models, as they grow in scale, continue to surprise us with utterly nonsensical and counterintuitive errors despite their otherwise remarkable performances on leaderboards. In this talk, I will argue that it is time to challenge the currently dominant paradigm of task-specific supervision built on top of large-scale self-supervised neural networks. I will first highlight how we can make better lemonade out of neural language models by shifting our focus on unsupervised, inference-time algorithms. I will demonstrate how unsupervised algorithms can match or even outperform supervised approaches on hard reasoning tasks such as nonmonotonic reasoning (such as counterfactual and abductive reasoning), or complex language generation tasks that require logical constraints. Next, I will highlight the importance of melding explicit and declarative knowledge encoded in symbolic knowledge graphs with implicit and observed knowledge encoded in neural language models. I will present COMET, Commonsense Transformers that learn neural representation of commonsense reasoning from a symbolic commonsense knowledge graph, and Social Chemistry 101, a new conceptual formalism, a knowledge graph, and neural models to reason about social, moral, and ethical norms.
  bio: Yejin Choi is a Brett Helsel associate professor at the Paul G. Allen School of Computer Science & Engineering at the University of Washington and also a senior research manager at AI2 overseeing the project Mosaic. Her research interests include commonsense knowledge and reasoning, neural language (de-)generation, language grounding, and AI for social good. She is a co-recipient of the AAAI Outstanding Paper Award in 2020, Borg Early Career Award (BECA) in 2018, IEEE’s AI Top 10 to Watch in 2015, the ICCV Marr Prize in 2013, and the inaugural Alexa Prize Challenge in 2017.
  video: https://www.youtube.com/embed/YITTv93xHXU
  UID: "3"
- title: Social Reinforcement Learning
  speaker: Natasha Jaques
  institution: Google Brain
  date: Dec 21, 2020
  abstract: Social learning helps humans and animals rapidly adapt to new circumstances, and drives the emergence of complex learned behaviors. This talk focuses on Social Reinforcement Learning, developing new RL algorithms that leverage social learning to improve single-agent learning and generalization, multi-agent coordination, and human-AI interaction. We will demonstrate how a multi-agent technique for Adversarial Environment Generation based on minimax regret can lead to the generation of a complex curriculum of training environments, which improves an agent’s zero-shot transfer to unknown, single-agent test tasks. To improve multi-agent coordination, we give agents an intrinsic motivation to increase their causal influence over the actions of other agents, and show that this leads to the emergence of communication and enhances cooperation. Finally, we propose a novel Offline RL technique for learning from intrinsic social cues during interaction with humans in an open-domain dialog setting. Together, this work argues that Social RL is a valuable approach for developing more general, sophisticated, and human-compatible AI.
  bio: Natasha Jaques holds a joint position as a Research Scientist at Google Brain and post-doc at UC Berkeley. Her research focuses on social reinforcement learning---developing multi-agent RL algorithms that can improve single-agent learning, generalization, coordination, and human-AI collaboration. Natasha received her PhD from MIT, where she focused on Affective Computing and new techniques for deep/reinforcement/machine learning. Her work has received the best demo award at NeurIPS 2016, best paper at the NeurIPS ML for Healthcare workshop, and an honourable mention for best paper at ICML 2019. She has interned at DeepMind, Google Brain, and is an OpenAI Scholars mentor. Her work has been featured in Quartz, the MIT Technology Review, Boston Magazine, and on CBC radio.  Natasha earned her Masters degree from the University of British Columbia, and undergraduate degrees in Computer Science and Psychology from the University of Regina
  video: https://www.youtube.com/embed/traKBhJm4lQ
  UID: "2"
- title: Promise, Progress, and Challenges in Open-Ended Machine Learning
  speaker: Joel Lehman
  institution: OpenAI
  date: Nov 23, 2020
  abstract: Researchers in open-ended machine learning are inspired by natural and human processes of innovation (like biological evolution or science itself), and aim to uncover the engineering principles underlying boundlessly creative search algorithms, i.e. algorithms capable of continual production of useful and interesting innovations, The speculative promise for machine learning from such open-ended search includes automated discovery of curricula for reinforcement learning, new neural architectures, and most ambitiously, AGI itself. While its most ambitious potential is far from being realized, conceptual and algorithmic progress has been made. I will review progress in open-ended search and highlight open challenges.
  bio: Joel Lehman is a research scientist at OpenAI, and previously was a founding member of Uber AI Labs and an assistant professor at the IT University of Copenhagen. His research focuses on open-endedness, reinforcement learning, evolutionary computation, and AI safety. His PhD dissertation introduced novelty search, an influential method within evolutionary computation, which inspired a popular science book co-written with Ken Stanley on what search algorithms imply for individual and societal objectives, called “Why Greatness Cannot Be Planned.”
  video:
  UID: "1"
